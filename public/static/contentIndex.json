{"index":{"title":"Welcome to my blog","links":[],"tags":[],"content":"Who am I?\nSite Reliability Engineer, Gamer &amp; Neovim Fanboy.\nWelcome to my Personal Site.\nIn here I will attempt to write about things I‚Äôm either working on or things I‚Äôm trying to learn to improve my skills, most of the times it‚Äôs just me trying out things that I find interesting or cool to demo.\nA little bit about me‚Ä¶ my name is Miguel, was born in Mexico and I‚Äôm an Electronic Engineer by trade that found a burning passion for software. Been in love with computers ever since my parents got a PC back when Jill of the Jungle was released, so yeah I was put on a path very early in my life. Can‚Äôt complain tho, really love what I do for a living and the world of IT overall.\nI‚Äôm an video game fan, most of the games I enjoy are FPS, Hack &amp; Slash and of course MOBAs. My favorite games are Diablo II and DoTA."},"posts/ansible-101":{"title":"Ansible 101","links":[],"tags":["automation","ansible"],"content":"Overview\nAnsible is my favorite configuration management tool, its uses YAML and Python, so you have a winning combo to begin with. One of the main things of why I enjoy using it, is basically that it doesn‚Äôt require an installation on the target servers and that is a big factor that in my opinion defeats other management tools like Chef or Puppet.\nThe fact that you can run and configure servers without anything installed on them besides python (which is almost now standard on every single virtual/physical machine).\nFew things to note:\n\nRequires python on both master and client machine\nConnects mostly over SSH so credential setup is needed\nUses or executes in sequence (top to bottom)\n\nAs we talked before, Ansible relies on YAML.\nYaml Aint Markup Language\nMeant to be ‚Äúhuman-readable‚Äù Perfect for the simplicity that involves ansible/python YAML uses a dictionary type of input information example: key:value Name: Mike\nYAML Examples (not directly ANSIBLE examples)\n--- #Pending Items\n  - Visa Renewal\n  Description: This is needed to renew your visa and this contains all of the information needed\n  Type: Important\n  Due: Today\n  Followups:\n  - CAS Appointment\n          Date: April 30th\n  - Consulate\n          Date: April 31st\n \nSame as with Python, indentation is critical for your YAML files.\nTo use ansible it‚Äôs as simple as: (process may be different based on your distribution)\n    sudo apt-get install ansible\nOnce installed you can start creating what we call ‚Äúplaybooks‚Äù, which is basically YAML instructions for Ansible to execute something. We will begin with the classic hello word. Create a file named hello.yml.\nhello.yml\n \n--- # Hello World\n- hosts: localhost\ntasks:\n    - debug:\n        msg: &quot;Hello World&quot;\nTo run your playbook execute the following command:\nansible-playbook hello.yml\n \nWill produce the following output:\n \nPLAY [localhost] *****************************************************************\n \nTASK [Gathering Facts] ***********************************************************\nok: [localhost]\n \nTASK [debug] *********************************************************************\nok: [localhost] =&gt; {\n    &quot;msg&quot;: &quot;Hello World&quot;\n}\n \nPLAY RECAP ***********************************************************************\nlocalhost                  : ok=2    changed=0    unreachable=0    failed=0\n \n \nAs you can see, running Ansible playbooks is incredibly easy and of course there a lot more topics to cover for Ansible basics. If you want to keep learning more, head over to the Ansible Documentation."},"posts/atlantis":{"title":"Terraform Automation with Atlantis","links":[],"tags":["automation","iac"],"content":"In big enterprises with decent IaC experience and infrastructure footprint you as a good engineer will use some sort of workflow to allow teams to deploy their infrastructure as code, so that things are controlled, centralized and manageable for whoever is paying the cloud bill.\nSo with that in mind there are a lot of companies and products that can help you keep that terraform under control (speaking of which, with the new License changes those companies are in trouble, enter ‚ÄòOpenTofu‚Äô that‚Äôs a story for another day), one of the tools that I‚Äôm familiar with and enjoy using is Atlantis which takes care of planning and applying as long as conditions are met.\nYou will ask yourself, now that sounds pretty cool, but I‚Äôm just a guy with a bunch of computers in the basement why would I care about IaC and automated workflows with fancy tools?\n\nTo which I will argue, yeah it‚Äôs kind of redundant when you are the sole user of your infrastructure but one thing to remember is that us techies like to play with new toys. So in this post we are going to setup Atlantis to automate my Homelab shenanigans.\nHow does Atlantis Work?\nGood question, well it basically connects to your GitHub/Azure/Gitlab instance or account and listens for events on certain repositories. Once it detects a change it will kick-start an execution of terraform on whatever path the project resides.\nWith the plan complete, it will update your PR with a comment giving you the details of what will be created. If everything looks good you can then ‚Äúapprove‚Äù the changes and Atlantis will apply them as planned.\nI‚Äôm sold what do I need?\nI won‚Äôt bore you with the details as the documentation will do it way better than I can, but long story short you need to acquire couple tokens so your Atlantis instance can connect safely to GitHub to listen for events.\nDocumentation can be found here\nIf you like to run things in containers like I do., his is how my yaml looks like, minus the secrets of course! Keep those hidden!. Feel free to use it as a base.\natlantis:\n    image: ghcr.io/runatlantis/atlantis\n    environment:\n    - ATLANTIS_GH_USER=&lt;your-gh-user&gt;\n    - ATLANTIS_GH_TOKEN=&lt;your-token-from-gh&gt;\n    - ATLANTIS_REPO_ALLOWLIST=github.com/mvaldes14/terraform || &lt;your-repo&gt;\n    - ATLANTIS_ATLANTIS_URL=atlantis.mvaldes.dev || &lt;your-available-instance-dns&gt;\n    - ATLANTIS_GH_WEBHOOK_SECRET=&lt;your-webhook-secret&gt;\n    - ATLANTIS_EMOJI_REACTION=thumbsup\n    - ATLANTIS_API_SECRET=&lt;random-secret-for-atlantis-to-validate&gt;\n    - ATLANTIS_TFE_TOKEN=&lt;your-hcp-cloud-token&gt;\n    - ATLANTIS_TFE_LOCAL_EXECUTION_MODE=true # Enable or disable as needed\n \n\nHaving a good repo structure is crucial to having a good workflow experience, otherwise you are not going to have a good time.\n\nWith everything running now you need to configure your repo with a proper atlantis.yml, so it knows where to go and what to read. In my case I have separate state and manifests depending on which ‚Äúapp‚Äù they belong to.\nYou can also split this by environments, providers or vendors. Again the key is to have a good structure, so you can manage it.\nHere‚Äôs my repository if you want to see the layout.\nExample of how my atlantis.yml looks like:\nversion: 3\nautomerge: true\ndelete_source_branch_on_merge: true\nprojects:\n  - name: grafana # Project Grafana\n    workspace: grafana # Each Project has it&#039;s own workspace\n    dir: apps/grafana # What folder to monitor\n    autoplan:\n      when_modified:\n      - &quot;*.tf&quot; # Which files types will trigger a plan,\n        # Useful to prevent executions on README or makefiles.\n  - name: aws  \n    workspace: aws\n    dir: apps/aws\n    autoplan:\n      when_modified:\n      - &quot;*.tf&quot;\n \nNOTE: If you like to store your state in HashiCorp Cloud like I do, your apply command needs to be different cause of a bug with remote execution. You can set your execution to local, but you will need to pass in the secrets/variables to atlantis in order for it to work.\n\nYou can expand on the plan details to see what resources will be created. And if everything pleases you, simply put a comment in the PR saying atlantis apply -- -auto-approve, and it will create the resources and also merge the PR, cause that‚Äôs what the configuration says remember?\n\nIt is very important you auto-approve it, since HCP forcefully wants you to use their UI to approve any changes before they can apply it so it kind of breaks the nice flow you can get with Atlantis, see issue for more details\nIf everything was done right, your PR will be merged and your infra should be ready for you.\nConclusion\nIt is very important to design your IaC pipeline ‚Äúcorrectly‚Äù and how you will manage it. So put these in a balance and pick your poison.\n\nDo I want/need to store the state in s3?\nIf not, do I even want to worry about state?; It‚Äôs the most complex part about terraform IMO.\nHow will I manage the secrets for the execution?; Those AWS Keys and Tokens have to be safe and providing them on each execution is not ideal.\nHow will I separate my resources, so I don‚Äôt end up with a state file having 1203019809 resources?\nWho will review and approve my changes before they go live?; Do we just allow anyone to create infra?\n\nFor your Homelab none of this would potentially apply, but now that you have an idea how pipelines for IaC with Terraform work, you might figure out how to apply these at your work and expand on them.\nWe didn‚Äôt go over variables and secrets, but maybe that‚Äôs another post down the road.\nHope you liked it!.\nAdios üëã"},"posts/blog-reborn":{"title":"Blog 3.0","links":[],"tags":["random"],"content":"Hello 2023!\nAlright so this is going to be the same thing I do every year, where I try to commit to doing more blog posts and expanding my knowledge while sharing it with whoever may end up reading this.\nBut this time I even went ahead and took like a week to actually rewrite it all from scratch using Astro, you should check it out by the way,\nWhich is a pretty trendy tool/design that allows you to build things for the web. So this helps cover that FOMO and also gave me an excuse to simplify the process, as the old blog had a bit of a mess with a complex Makefile that would break the commits and the configuration for the engine that rendered everything into HTML was sort of complex. With Astro everything is pure JSX along with MarkdownX, so it‚Äôs far simpler in my opinion.\nThe extra one thing I want to mention about Astro before I move on is that the deployment process is fully automated via GitHub Actions, which is pretty damn great!\nEither way, this upcoming year I do intend to write a bit more and share it here.\nThe design of the site may also change from time to time since I‚Äôve got more control over it and the entire template is something custom-made, that‚Äôs also the reason of why it may look ugly or may not work well with mobile devices, but I‚Äôm gonna work on it, pinky promise.\nLast but not least I do have a range of topics and things I want to explore and write about some of them being:\n\nHomelabs\nAutomation with Chef or Ansible\nActually use Kubernetes at home more\nLearning new languages while building things with them, looking at you Typescript and Golang\nCompare and talk about some of the observability tools that everyone is using\nNeovim which is what I‚Äôve been using for everything for the past 6 months now\n\nThat is it for now, thanks again for reading.\nAdios!"},"posts/bootstrap-ansible":{"title":"Bootstrap your system with ansible","links":[],"tags":["automation","ansible"],"content":"I‚Äôve been distro hoping for a month or so now trying to find the perfect balance between productivity and ease of use as well as aesthetics, so I could feel comfortable using my computer for more than just messing around (Ended up staying with Manjaro KDE in case you were curious).\nAnyways, doing so made me install over and over again some packages and apps every single time. I know I could‚Äôve backed up the entire /home partition but there were still some dependencies and libraries needed so instead of doing this repetitive process, figured I‚Äôd use automation tools, like ansible.\n\nFirst, i came up with a list of everything i had installed already. Depending on your distro you can check your /var/log/pacman.log or do an extract using pacman -Ql.\nNext was to create a repo that would hold my special application configuration files as well as the playbooks so I can simply copy those into my new system and just run them.\nFinally, create an ansible role structure for all the different things I‚Äôve wanted to install, I‚Äôm a fan of keeping things tagged and organized as well as having some specific things installed in specific devices, like steam and my games are useless in my personal laptop so having things separated by roles helps a ton.\n\nWith the list and structure ready, ended up with something like this.\n--- # Main playbook\n- hosts: local\n  gather_facts: no\n  connection: local\n  roles:\n    - aur\n    - base\n    - python\n    - games\n    - pi\n    - snaps\nSo all my tasks were separated based on the source or module being used by them. Do note that in my case since I‚Äôm an Arch user, had to install a module for ansible in order for me to get things from the AUR. Ended up using github.com/kewlfft/ansible-aur. The rest of the modules are built-in assuming you have Ansible v2.8.\nAll that was left was to define the tasks inside each role and start running things. If you want to see the full thing, refer to my dotfiles. But here‚Äôs a brief example of what I consider my base packages.\n--- # tasks file for base items\n- name: Install applications from pacaur\n  become: yes\n  become_method: sudo\n  pacman:\n    name: &#039;{{ item }}&#039;\n    state: present\n  loop:\n    - rofi\n    - imagemagick\n    - docker\n    - jq\n    - neovim\n    - nodejs\n    - npm\n    - playerctl\n    - vagrant\n    - virtualbox\n    - lxappearance\n    - chromium\n    - neofetch\n    - postman-bin\n    - rsync\n    - rclone\n    - spotify\n    - visual-studio-code-bin\n    - mpv\n    - xclip\n    - redshift\n    - fzf\n    - httpie\n    - zeal\n    - lastpass-cli\n    - nmap\n    - prettyping\n    - bat\n    - ncdu\n    - python-pip\n    - pavucontrol\n    - pulseaudio-alsa\n    - adobe-source-code-pro-fonts\n    - noto-fonts-emoji\n    - yay\n    - dropbox\n    - konsole\n    - transmission-gtk\n    - clementine\n    - krusader\n    - wps-office\n    - zsh\n    - go\n    - terraform\n    - firefox\n    - filezilla\n \n    - arch-base\nOne thing I would‚Äôve loved would be a way to run just specific roles but that seems to be not doable right now so we use the next best thing, tags in the playbook tasks.\nSo for example if i just want to run the playbook to install things from AUR.\n- name: Install from AUR\n  aur:\n    use: yay\n    skip_installed: yes\n    aur_only: yes\n    name:\n      - ttf-font-awesome-4\n      - snapd\n      - ttf-material-design-icons\n      - mailspring\n      - wireshark-gtk\n      - android-studio\n      - bind\n      - slack-desktop\n      - discord\n      - tor-browser\n \n      - arch-aur\nIt can be run with something like.. ansible-playbook main.yml -t arch-aur -K\nBreaking it down:\n\nmain.yml: The name of the principal YML file that contains all roles.\n-t arch-aur: We specify that we only want to run the portion where the tag is arch-aur.\n-K: to provide our root password so that the administrative actions can take place.\n\nSimple as that i can have a fully bootstrapped system ready to go in minutes, this project was extremely helpful for the distro hoping portion as mentioned in the beginning but since i moved away from windows on my laptop and honestly didn‚Äôt want to spend a lot of time doing the whole thing, the playbook schema worked out perfectly. Also having the tags for different systems i could specify what to run where.\nAt this stage, the execution of those filters is done manually by using tags but you can improve on the playbook by comparing the name of the system and just executing certain roles based on conditions, in my case since i don‚Äôt plan to do the installation for a while now i can live with manual portion. Next stop would be to find a way for me to keep the list updated every time i install something from either pacman, aur, snap, etc. Guess that will be the next project for the future.\nHope you liked it, see you next time."},"posts/chef-tips":{"title":"Chef Tips and Tricks","links":[],"tags":["chef","automation"],"content":"Lately I‚Äôve been pretty deep into the Chef weeds and the more I end up working on it, the more I keep finding these little tips and tricks on how to get something done, some of these come from Seniors that passed them on to me and some of them I either end up finding online or figuring them out so would like to share them in case someone finds them useful.\nNeed to run a command and use the output for something?\nThere is the execute or ruby_block resources but what if you need the output of something to determine if a resource should run or not?. Maybe its a guard for another resource in your recipe, simple, use the mixlib/shellout library. Should be installed since it‚Äôs part of Chef SDK.\nrequire &#039;mixlib/shellout&#039;\nfind = Mixlib::ShellOut.new(&quot;find . -name &#039;*.rb&#039;&quot;)\nfind.run_command\n \n # Grab the output either good or bad\nputs find.stdout\nputs find.stderr\nCopy a local cookbook to a node with chef installed\nIf you ever need to see how your cookbook will be applied to a machine but you are not ready yet to push it out to your chef server?. I gotchu, you can simply zip the entire cookbook and copy it over to a node and then apply a new runlist!.\n\nCopy the tarball and put it somewhere\nModify the client.rb to point to a directory holding the cookbooks (it must contain the metadata.rb)\nEdit the client.rb so it fetches from a local path, the cache location is a good spot since it already has your cookbooks.\n\n  cookbook_path    /var/cinc/cache/cookbooks\n\nRun cinc client in solo mode and specify the runlist, it is important you run this with -z so its done in solo mode. Meaning it won‚Äôt reach out to the chef server to fetch data.\n\ncinc-client -z -r &quot;yourcookbook::recipe&quot;\nYour cookbook should now be applied to the instance, hack away!\nTest a resource before putting it in a cookbook\nIn a node with cinc installed you can enter into the shell and go inside recipe_mode to test out resources. If you are happy with them you can later run run_chef to actually execute them against a node. This is great to test and design cookbooks.\ncinc-shell -s\n \ncinc (17.9.26)&gt; recipe_mode\ncinc:recipe &gt; git &#039;/tmp/dotfiles&#039; do\ncinc:recipe &gt;   repository &#039;github.com/mvaldes14/dotfiles.git&#039;\ncinc:recipe (17.9.26)&gt; end\n =&gt; &lt;git[/tmp/dotfiles] @name: &quot;/tmp/dotfiles&quot; @before: nil @params: {} @provider: nil @allowed_actions: [:nothing, :sync, :checkout, :export, :diff, :log] @action: [:sync] @updated: false @updated_by_last_action: false @source_line: &quot;(irb#1):1:in `&lt;main&gt;&#039;&quot; @guard_interpreter: nil @default_guard_interpreter: :default @elapsed_time: 0 @declared_type: :git @cookbook_name: nil @recipe_name: nil @repository: &quot;github.com/mvaldes14/dotfiles.git&quot;&gt;\ncinc:recipe (17.9.26)&gt; run_chef\n[2022-10-12T20:56:21-05:00] INFO: Processing git[/tmp/dotfiles] action sync ((irb#1) line 1)\n[2022-10-12T20:56:22-05:00] INFO: git[/tmp/dotfiles] cloning repo github.com/mvaldes14/dotfiles.git to /tmp/dotfiles\n[2022-10-12T20:56:23-05:00] INFO: git[/tmp/dotfiles] checked out reference: 5c362e70aaa0c51055df0c7015582d89ab3e1017\n =&gt; true\n \nSee attributes on instance\nThis is useful to debug if an attribute does not behave as you expected or if you have a lot of overrides and don‚Äôt know which one is the final one being kept. Also helpful in case ohai replaces something you were expecting.\nIf its in a kitchen converge do\ncd /tmp/kitchen\ncinc-shell -c client.rb -j dna.json\nnode[&#039;attribute&#039;]\nIf it is an already bootstraped instance then\ncinc-shell -z\nnode[&#039;attribute&#039;]\nRun systemd inside of test kitchen with dokken\nThis is pretty useful if you are not using vagrant to test out your kitchen instances and prefer something quicker like Docker, which works great except when you need to interact with systemd to actually start/enable a service you just installed.\nFor those scenarios you can simply pass or mount part of your cgroups so systemd runs inside the container.\nNOTE: This only applies if you are using NIX as the base machine to develop your cookbooks, for other OS I honestly have no clue‚Ä¶ maybe just use vagrant.\nYour kitchen.yml should looks something like this.\ndriver:\n  name: dokken\n \ntransport:\n  name: dokken\n \nprovisioner:\n  name: dokken\n \nverifier:\n  name: inspec\n \nplatforms:\n  - name: centos-7\n    driver:\n      image: centos:7\n      privileged: true\n      pid_one_command: /usr/lib/systemd/systemd\n      volume:\n        - /sys/fs/cgroup:/sys/fs/cgroup:ro\nThose are some of my most precious tips so hopefully they serve you well.\nIf you got more tips on how to do magic things in Chef please do let me know so I can learn and include them into my blogs and wikis!.\nUntil the next one, adios üëã"},"posts/cloudflare-tunnels":{"title":"Cloudflare Tunnels are the bomb","links":[],"tags":["homelab","k8s"],"content":"As a self-hosted user, I understand the importance of keeping my applications and sites secure while still being able to access them from anywhere, because not everything can depend on me being on my home LAN.\nThat‚Äôs why I started using Cloudflare Tunnels - a service that allows me to securely expose my self-hosted applications to the internet without compromising my security and it works perfectly in combination with my reverse Proxy so all I need to expose is one entry point and the proxy does the rest and provides the SSL certificates.\nI also appreciate how scalable Cloudflare Tunnels is. As my self-hosted applications and traffic volumes grow, I know that Cloudflare Tunnels can handle the increased load without impacting my server‚Äôs performance. And with Cloudflare use of HTTP/2 and multiplexing, I know that my application performance will remain fast and efficient.\nThe Setup\nRunning a Cloudflare Tunnels was straightforward and easy to follow, can be done in minutes. I simply had to download and install the Cloudflare Tunnels client on my server and create a configuration file specifying the local port I wanted to expose and the hostname I wanted to use.\nAfter running the command to establish the tunnel, I was able to start forwarding traffic to my self-hosted applications. Once you validate everything works as expected don‚Äôt forget to either create a service so it can start on boot OR even better let the Cloudflare binary do it for you.\nI‚Äôm not going to cover how to download the binary or setting up the account so, I‚Äôm going to refer you to the original docs here.\nYou should review them if you have multiple domains or a more advanced use case but for simple self-hosted users like myself that just want to watch some videos via Jellyfin or get to my NextCloud.\nHere‚Äôs how my configuration ended up looking like.\ntunnel: &lt;ID-OF-MY-TUNNEL&gt;\ncredentials-file: &lt;PATH-TO-MY-CREDENTIALS-FILE&gt;\n \ningress:\n  - hostname: &#039;*.mvaldes.dev&#039; # Domain you want to allow\n    service: https://localhost # Where to send it, in my case port 443 to my localhost which is where my proxy runs\n    originRequest:\n      noTLSVerify: true # For my Lets Encrypt certs\n  - service: http_status:404 # Where to send you if you go to a domain that doesn&#039;t exist\nCouple things to note:\n\nYou need an account with Cloudflare to set up your tunnel, those are free so go get one\nYou need a domain to forward the traffic to\nYou need your proxy listening for connections on all IPs, aka having it configured as 0.0.0.0\n\nDon‚Äôt forget the service unit so it auto starts.\n[Unit]\nDescription=cloudflared\nAfter=network.target\n \n[Service]\nTimeoutStartSec=0\nType=notify\nExecStart=/home/$USER/apps/cloudfared/cloudflared --no-autoupdate --config /etc/cloudflared/config.yml --metrics 0.0.0.0:3001 tunnel run\nRestart=on-failure\nRestartSec=5s\n \n[Install]\nWantedBy=multi-user.target\nPart of the documentation explains how you can expose metrics from the tunnel in a Prometheus format so of course I had to set that up.\nCan‚Äôt say no to a Grafana dashboard am I right?!?!\n\nAs you can tell from the graph and metrics my usage is very lightweight considering it‚Äôs for my personal use, but this thing should be able to handle heavier workloads.\nOne cool thing it can do is also allow you to remote SSH into your machines, but I‚Äôm scared of even trying that, also I have no real need for it.\nFor accessing my machines there is always WireGuard\nIn conclusion\nOne of the benefits of Cloudflare Tunnels that stood out to me was the protection you get from the service out of the box like DDoS attacks and can even be combined with other offerings like Firewalls.\nI could be wrong on this one, but this will also help you a lot if you don‚Äôt have a static IP from your ISP, meaning you can set this up anywhere.\nOverall, Cloudflare Tunnels has been a great solution for me as a self-hosted user. It has allowed me to securely expose my applications to the internet without compromising my security or performance. With its ease of use, advanced security features, and scalability, I highly recommend Cloudflare Tunnels to any self-hosted user looking for a secure and reliable way to access their applications from anywhere.\nThis is a better solution than forwarding ports from your router to your machines, please don‚Äôt do that.\nUntil the next one, adios üëã"},"posts/covid-dashboard":{"title":"COVID Data in Elasticsearch & Kibana","links":["[https:/github.com/CSSEGISandData/COVID-19](https:/github.com/CSSEGISandData/COVID-19)"],"tags":["elk"],"content":"So as most people I‚Äôve been stuck at home so it gave me some extra time to tinker around the dataset on the excellent dashboard by Johns Hopkins CSSE you should check that out if you haven‚Äôt already, it‚Äôs quite popular. Anyways found out that they publish all of the data behind the dashboard to their Github so wrote a quick script to pull the csv files for March, transform the data via Logstash and push it into my local Elasticsearch instance.\nGetting and formatting data\nPulling data\nWith the help of the python requests library it was simple to pull the data for each day and just dump it into a file so that i could later tweak it.\ndef get_files(day):\n    # Downloads the files from the REPO and places them in the data/raw folder\n    r = requests.get(f&quot;raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports/{day}.csv&quot;)\n    file = r.text\n    with open(f&#039;./data/raw/{day}-raw.csv&#039;,&#039;w&#039;) as infile:\n        for line in file:\n            infile.write(line)\nFormatting the data\nWhile reviewing the csv data, I noticed that there were ‚Äúgaps‚Äù in between lines for some records, mostly missing states or province so I just read each line and if the province was missing, I copied whatever the line had for Country.\n\nQuite simple to do with the csv DictReader function.\ndef format_files(day):\n    # Swaps the column order and fills out missing data for countries and states\n    with open(f&#039;./data/raw/{day}-raw.csv&#039;, encoding=&quot;utf-8-sig&quot;) as infile, open(f&#039;./data/{day}.csv&#039;, &#039;w&#039;) as outfile:\n        reader = csv.DictReader(infile)\n        headers = [&quot;Country/Region&quot;,&quot;Province/State&quot;,&quot;Last Update&quot;,&quot;Confirmed&quot;,&quot;Deaths&quot;,&quot;Recovered&quot;,&quot;Latitude&quot;,&quot;Longitude&quot;]\n        writer = csv.DictWriter(outfile,fieldnames=headers)\n        writer.writeheader()\n        for line in reader:\n            # Add Country if it doesn&#039;t exist\n            if not line[&quot;Province/State&quot;]:\n               line[&quot;Province/State&quot;] = line[&quot;Country/Region&quot;]\n            writer.writerow(line)\nWith the columns swapped and consistent, I had something nice and manageable.\nCountry/Region,Province/State,LastUpdate,Confirmed,Deaths,Recovered,Latitude,Longitude\nChina,Hubei,2020-03-17T11:53:10,67799,3111,56003,30.9756,112.2707\nItaly,Italy,2020-03-17T18:33:02,31506,2503,2941,41.8719,12.5674\nIran,Iran,2020-03-17T15:13:09,16169,988,5389,32.4279,53.6880\nSpain,Spain,2020-03-17T20:53:02,11748,533,1028,40.4637,-3.7492\nPushing the data to Elasticsearch\nThere are multiple ways to push data into an Elasticsearch instance, in previous posts I‚Äôve done it with the python library but I had a Logstash instance up and running so figured it was easier to use it to read all csv files in my desired location, run it through some of the filters and push it into the cluster for me if you are familiar with how Logstash work you can skip the breakdown.\nELI5 Logstash - Tool used to transform data, it basically consists of 3 blocks. An input to read data from. A filter to transform or alter the data. An output to send the transformed data to.\nFirst, we have to tell Logstash what we want to read since we had static files all I had to do was use the file module, all it requires is a path to read from. To prevent it from reading the files over and over it employs a ‚Äútracker‚Äù that keeps a record of which files were read up until what position. Filebeat does exactly the same and it keeps an internal registry.\ninput {\nfile {\n  path =&gt; [&quot;/home/&lt;user&gt;/projects/covid-dashboard/data/*.csv&quot;]\n  start_position =&gt; &quot;beginning&quot;\n  tags =&gt; [&#039;covid&#039;,&#039;dataset&#039;]\n  sincedb_path =&gt; [&quot;/home/&lt;user&gt;/projects/covid-dashboard/tracker&quot;]\n}\n}\n \nNext up we have to run every single record from each file through a series of filters, from decoding to changing the type of data so it can be used in Elasticsearch.\nfilter {\n  csv {\n   columns =&gt; [&quot;Country&quot;,&quot;State&quot;,&quot;LastUpdate&quot;,&quot;Confirmed&quot;,&quot;Deaths&quot;,&quot;Recovered&quot;,&quot;Latitude&quot;,&quot;Longitude&quot;]\n   skip_header =&gt; true\n   convert =&gt; {\n      &quot;Confirmed&quot; =&gt; &quot;integer&quot;\n      &quot;Deaths&quot; =&gt; &quot;integer&quot;\n      &quot;LastUpdate&quot; =&gt; &quot;date_time&quot;\n      &quot;Recovered&quot; =&gt; &quot;integer&quot;\n      &quot;Longitude&quot; =&gt; &quot;float&quot;\n      &quot;Latitude&quot; =&gt; &quot;float&quot;\n    }\n  }\n  mutate {\n    rename =&gt; {\n      &quot;Longitude&quot; =&gt; &quot;[Location][lon]&quot;\n      &quot;Latitude&quot;  =&gt; &quot;[Location][lat]&quot;\n    }\n    remove_field =&gt; [&quot;message&quot;,&quot;host&quot;,&quot;path&quot;,&quot;@timestamp&quot;,&quot;@version&quot;]\n  }\n}\nThe CSV block simply decodes each line and turns all of the records into Key-Value pairs, then uses the custom headers I wanted to name those keys. The second part turns some of the fields into integers and dates, I do want to point out that this didn‚Äôt work 100% of the time so I had to do a workaround at Elasticsearch which will be posted in here as well.\nThe mutate block creates a ‚Äúgeo_point‚Äù object that is a nested object that contains a latitude and longitude. It also removes some fields I didn‚Äôt feel were needed.\noutput {\n  elasticsearch {\n      hosts =&gt; &#039;http://localhost:9200&#039;\n      index =&gt; &#039;covid&#039;\n  }\n}\nFinally, we push out the data to Elasticsearch to an index called ‚Äúcovid‚Äù.\nAdjust the data in Elasticsearch\nAs mentioned above, I kept running into issues where some records could not be indexed cause of data type mismatch so after trying for couple hours ended up forcing Elasticsearch to do what I wanted by creating the mapping directly and applying it to the ‚Äúcovid‚Äù index. Templates are an Elasticsearch concept that‚Äôs incredibly powerful and everyone using it should know about it.\nMy template ends up looking like‚Ä¶\nPUT _template/covid\n{\n  &quot;order&quot;: 0,\n  &quot;index_patterns&quot;: [\n    &quot;covid&quot;\n  ],\n  &quot;settings&quot;: {\n    &quot;number_of_replicas&quot;:0\n  },\n  &quot;mappings&quot;: {\n    &quot;properties&quot;:{\n      &quot;Location&quot;: {\n        &quot;type&quot;: &quot;geo_point&quot;\n      },\n      &quot;Confirmed&quot;:{\n        &quot;type&quot;: &quot;double&quot;\n      },\n      &quot;Deaths&quot;: {\n        &quot;type&quot;: &quot;double&quot;\n      },\n      &quot;LastUpdate&quot;: {\n        &quot;type&quot;: &quot;date&quot;\n      },\n      &quot;Recovered&quot;: {\n        &quot;type&quot;: &quot;double&quot;\n      }\n    }\n  }\n}\nAs you can see I‚Äôm merely indicating how the data should look like in terms of the types.\nWith that in place, it was time to run Logstash and start pushing all 3k+ records, each record ended up looking like.\n&quot;_index&quot; : &quot;covid&quot;,\n&quot;_type&quot; : &quot;_doc&quot;,\n&quot;_id&quot; : &quot;9O8I73ABTHN1r9G_vStK&quot;,\n&quot;_score&quot; : 1.0,\n&quot;_source&quot; : {\n  &quot;Confirmed&quot; : 990,\n  &quot;Recovered&quot; : 917,\n  &quot;State&quot; : &quot;Anhui&quot;,\n  &quot;Deaths&quot; : 6,\n  &quot;LastUpdate&quot; : &quot;2020-03-02T15:03:23&quot;,\n  &quot;tags&quot; : [\n    &quot;covid&quot;,\n    &quot;dataset&quot;\n  ],\n  &quot;Country&quot; : &quot;Mainland China&quot;,\n  &quot;Location&quot; : {\n    &quot;lat&quot; : 31.8257,\n    &quot;lon&quot; : 117.2264\n  }\n \nExploring the data\nWith all of March records so far we can now start exploring the data, in my case I consume things visually so the first thing I did was to start plotting.\nI was curious to see how the number of confirmed cases spike in Italy so why not put it in a line chart?. It took off incredibly fast.\n\nI know that most of the casualties occurred in Washington State so the data in a heatmap.\n\nFinally, since we have coordinate we could, in theory, replicate some of the dashboards from Johns Hopkins, I‚Äôm aware the data needs tweaking to fully be a copy but this sort of gives us an idea.\n\nWith all records, you can explore further and ask all sorts of questions on which states have more cases, which ones are ‚Äúsafe‚Äù or quiet. If you are smart you could, use the data to start predicting how the numbers will look like in the coming weeks.\nConclusion\nAs always, hoped this kept you busy for a bit, I know it took me a couple hours to bootstrap this whole thing and play with the data/script.\nIf you have any questions reach out on social media - the repo for everything in this post can be found in Github\nOne last thing‚Ä¶ Stay at home folks and tend to your families, don‚Äôt be a dick."},"posts/docker-flask":{"title":"Python Flask in a container","links":[],"tags":["container"],"content":"Make your apps easier to deploy and carry with you\nI‚Äôve been recently using my work laptop to code while things are calm and I‚Äôm waiting for the next fire to pop up (I work in production support). So once i installed everything I needed‚Ä¶. python, git, vscode, nodejs, etc. Realized one big thing, everything works differently in windows and I‚Äôm already used to work in Linux OS systems and i cannot just switch the OS in my work laptop cause then the IT guys going to get me fired.\nSo while looking at solutions to mitigate my situation, decided to force myself to use something new and exciting because it would‚Äôve been easy to just install Vagrant/VirtualBox and spin a machine to do my dev work but where is the fun in that?\nEnter Docker‚Ä¶\nIn this new IT world where everything is moving towards containers and microservices, thought it would be a good idea to jump on the hype train and learn how to ‚Äòdockerize‚Äô my flask applications. So I watched some videos and tutorials, read some of the documentation and did a test on a dummy application, so here we go.\nInstall Docker Engine First thing is of course installing Docker, now depending on your OS you can get it one way or another. Unless you have an enterprise need or license, go with the CE versions. Docker install\nCreate or use your application In my case, most of my applications were either running on my local rasperby pi at home or on heroku so I just decided to create a super simple dummy app that would display a picture and some text.\nSo once i created a new virtualenv using pipenv, my folder structure looked like this.\n```python\n.\n‚îú‚îÄ‚îÄ Pipfile\n‚îú‚îÄ‚îÄ Pipfile.lock\n‚îî‚îÄ‚îÄ python-flask\n    ‚îú‚îÄ‚îÄ app.py\n    ‚îú‚îÄ‚îÄ requirements.txt\n    ‚îî‚îÄ‚îÄ templates\n        ‚îî‚îÄ‚îÄ index.html\n\napp.py\n```python\nfrom flask import Flask, render_template\nimport random\n\napp = Flask(__name__)\napp.config[&#039;SECRET_KEY&#039;] = random.randint(1, 100)\n\n@app.route(&#039;/&#039;)\ndef index():\n    return render_template(&#039;index.html&#039;)\n\nif __name__ == &#039;__main__&#039;:\n    app.run(debug=True)\n\nAs you can see an incredibly simple app that returns an even simpler ‚Äòindex.html‚Äô.\nindex.html\n&lt;!DOCTYPE html&gt;\n&lt;html lang=&quot;en&quot;&gt;\n&lt;head&gt;\n    &lt;meta charset=&quot;UTF-8&quot;&gt;\n    &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;\n    &lt;meta http-equiv=&quot;X-UA-Compatible&quot; content=&quot;ie=edge&quot;&gt;\n    &lt;style&gt;\n        .body {\n            background-color: magenta;\n            margin: 20px;\n            padding: 20px;\n        }\n        .h1 {\n            color: pink;\n        }\n    &lt;/style&gt;\n    &lt;title&gt;Document&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Docker&lt;/h1&gt;\n        &lt;p&gt; Container python app &lt;/p&gt;\n    &lt;hr&gt;\n    &lt;img src=&quot;www.imagefully.com/wp-content/uploads/2015/08/Funny-Cats-Lol-Sup-Bro-Image.jpg&quot; alt=&quot;&quot;&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n\n** Create your Dockerfile **\nNow, in order for you to interact with the Docker Engine you need to instruct docker how to build your image. An image can be seen as a snapshot of how something should look like including configurations, files, environment variables, etc.\nThis is the part were I got stuck the most since I had to read what the keywords do and how to interact with them, so if you want the reference for everything, check here.\nWill give you a summarized version of what I did for my personal image.\nAll dockerfiles must either be using an existing base image or using something from scratch.\nSince the docker store already contains a bi-zillion images from official repos like Ubuntu, centos, nginx, mysql, etc. There should be no need for you to create something from scratch were you basically build the OS layers and everything. So in my case i went with a very simple Ubuntu image.\nThis is how you ‚Äòinherit‚Äô or use the base image from the docker store.\nFROM ubuntu:latest\n\nIf you want to see all of the images in your system simply run the command docker images. In my case i downloaded and played around postgress as well so my output looked like.\nREPOSITORY           TAG                 IMAGE ID            CREATED             SIZE\ndocker.io/ubuntu     latest              0458a4468cbc        12 days ago         111.7 MB\ndocker.io/postgres   9.6-alpine          7470b931fc2e        4 weeks ago         37.82 MB\n\nAs you can see, an ubuntu image is 111.7 MB ONLY!!! - This to me is the beauty of the containers, incredibly light weight and super portable for you to carry around, if we were doing this with a Virtual Machine, we would be talking about gbs of data.\nNext command on a typical file are labels, which can contain anything you want, mostly used for metadata.\nLABEL maintainer=&quot;yourname&quot; version=&quot;1.0&quot; maintainer_email=&quot;youremail@mail.com&quot;\nNOTE: Most images still use the deprecated keyword MAINTAINER. Ideally you want to use LABELs instead.\n\nNext command is super important as it allows you to specify what you want docker to do while building your image, this is where you typically install or do things on top of base images.\nIn my case I wanted to install pip and the build-essential bundle so I could run my flask application.\nRUN apt-get update -y &amp;&amp; apt-get install -y python-pip python-dev build-essential\n\nIdeally you want to ‚Äòchain‚Äô your commands using ‚Äò&amp;&amp;‚Äô so you reduce the amount of layers it generates.\nOnce you have everything installed you want to move your application code into the image to be used, so in here based on what I read you can either use COPY or ADD. You might want to read the documentation to see which one to pick but based on this SO post. For basic data moving, either one will work just fine.\nCOPY ./python-flask/ /usr/src/app\n\nAs most python applications, we requires modules and packages to run things, unless you are using the built-in library of course. But we doing flask, we big boys now so we need to install the packages and its dependencies. We simply add another layer that you will most likely recognize.\nRUN pip install -r /usr/src/app/requirements.txt\n\nWe run it at this point since the files were just copied a line above. Remember our requirements.txt was inside the application folder. You can modify the structure and alter the layers but you need to install your requirements at some point.\nOnce everything is copied and installed we need to tell Docker where we will run our things, so we make use of WORKDIR. It basically sets the directory where you will run your commands from. If you are running a binary that is available in your $PATH then you may not need this but since i want to make sure my application launches and uses the code we copied above, I force the location.\nWORKDIR /usr/src/app\n\nA key thing with containers is that, they create and spawn the process you tell them to but if you need to interact with it, you need a port to talk to. So in our case, since by default all flask apps run on port 5000. We tell Docker that we want to expose that port in our image so we can actually interact with it.\nEXPOSE 5000\n\nFinally, we run the application.\nCMD [&quot;flask&quot;,&quot;run&quot;,&quot;--host=0.0.0.0&quot;]\n\nDo note we are using CMD instead of RUN. This is because we just want this command to be executed as soon as the container is launched, in our case we use the preferred form (called exec) of separating items into a list. You can also use it in a shell form (without the list [] and ‚Äù); Again for specifics the documentation does wonders.\nBuild your image\nOnce we have our files and structure ready, we need to build our image using the Dockerfile we created.\n.\n‚îú‚îÄ‚îÄ Dockerfile\n‚îú‚îÄ‚îÄ Pipfile\n‚îú‚îÄ‚îÄ Pipfile.lock\n‚îî‚îÄ‚îÄ python-flask\n    ‚îú‚îÄ‚îÄ app.py\n    ‚îú‚îÄ‚îÄ requirements.txt\n    ‚îî‚îÄ‚îÄ templates\n        ‚îî‚îÄ‚îÄ index.html\n\nTo build our image we interact with the docker API, or the CLI for mere mortals like me‚Ä¶ By using the following command:\ndocker build -t flask .\n\nThings to note. -t TAG Name to use for your image, in my case I‚Äôm saying flask so its easy to remember. . With the dot we specify that we want to build using the Dockerfile available in our current location, if you want to use the file from a different place just specify the path.\nOnce the process completes, if you re-run docker images you should see a new image in your repository.\nREPOSITORY           TAG                 IMAGE ID            CREATED             SIZE\nflask                latest              5394dbc7f0eb        23 hours ago        424.6 MB\ndocker.io/ubuntu     latest              0458a4468cbc        12 days ago         111.7 MB\ndocker.io/postgres   9.6-alpine          7470b931fc2e        4 weeks ago         37.82 MB\n\nAs you can see, our image increased quite a bit, but even with that size it can be up and running in seconds. So remember, the more you install and add to it, the bigger it cause, duh logic right?\nRun your image\nIf you made it this far, good for you mate. We are almost done. To run our image we again interact with the CLI but in here we need to add some specific parameters to tell it where to put our port and give it a name.\ndocker run -d \\\n-p 80:5000 \\\n--name flaskapp \\\n-e FLASK_APP=&#039;app.py&#039; \\\nflask\n\nThe above command should give you a container id, validate it is actually running by doing docker ps. Additionally you can see the usual flask logs by running docker logs .\nSince we passed the ‚Äîname parameter, we just do docker logs flaskapp. And we get the following.\n* Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n24.28.147.10 - - [14/Feb/2018 17:00:43] &quot;GET / HTTP/1.1&quot; 200 -\n\nIt is very important that the host is ‚Äò0.0.0.0‚Äô otherwise you will not be able to access it from the outside. If you want to know why, check out this link.\nFinally, if we hit the IP where the docker image is running, we should get our flask application. Questions? Concerns? Was this cool?\nLet me know in the section below. Next time we will use the image to do live testing in our code.\nCheers!"},"posts/docker-home-setup":{"title":"My Home Docker Setup[2020]","links":[],"tags":["container"],"content":"Recently decided to migrate all of the things that I run on my personal computer and a little server I got for Xmas (originally for PLEX) as well as my raspberry pi to docker containers so that I could experiment more with it and boy, not as easy as I thought it was going to be.\nMost of the applications had specific configuration files in a location that needed to be mounted somewhere, then I had to think of which apps to move to which node cause of the resources on each node, setup the permissions and respective volumes depending on the usage, all in all, it was fun and I learned a lot.\nOne last thing was that I knew very little how the networking aspect worked so I had to create networks between some of these so that I could have communication between them. For instance, the Kibana instance for my elastic cluster has a cross-cluster search so I can visualize the data from both clusters in a single pane so I had to figure out how to make these connections to each other.\nNOTE: Everything in here is using Docker SWARM, so make sure you have your swarm setup.\nContainers:\nElasticsearch clusters\n\n\nHome cluster - for things I play with (trump twitter data, couple indices that are used for an app I‚Äôm working on, etc.). The home cluster comes with an instance of Logstash and Kibana respectfully.\n\n\nServer cluster - for reading data that is being generated by a script I made (mostly to keep track of my internet speed, pihole logs), this cluster consists of a gateway node and a data node. One receives the traffic, the other one keeps the data.\n\n\nGuacamole\n\nUsed to remote access my servers that are online 24/7.\n\nKafka &amp; Zookeeper\n\nBeen playing with it a lot lately, not much else to say. Definitely a cool tool.\n\nLDAP\n\nI wanted to have some of my services use this as an authentication backend but haven‚Äôt had the time to create the schemas and add users, but its running.\n\nMedia\n\nCouple containers to keep track of series and TV shows I watch ( sonarr, radarr, jackett )\n\nMySQL\n\nLocal database so I can dump data sets for me to play with.\n\nOrganizr\n\nAn excellent application that allows you to manage all of your bookmarks, so now I can just click on an icon and I‚Äôm taken to a container.\n\nPortainer\n\nUsed to manage most of my stack, sometimes the GUIs are better than the terminal.\n\nPrometheus &amp; Grafana\n\nTo collect and graph all of the metrics from my systems.\n\nTraefik\n\nReverse proxy for my containers, no longer have to remember IPs and ports.\n\nGot couple more services running on the servers but due to hardware constraints, mostly video decoding for plex, those remain as system processes that I can turn on/off when needed.\nSame as my PiHole, I found that its easier to just keep it running on my raspberry pi. Might move it to a container soon using mcvlan.\nSetup\nMost containers need couple gigs of storage and some don‚Äôt require a thing as long as they have a mount for the config file so based on that I ended up creating a couple of mount points as follows:\n\n/opt/elasticsearch 50GB on both home and server nodes to allocate the index data.\n/opt/prometheus 1GB to hold the prometheus.yml required for the scrape targets as well as hosting the time series data collected, by default I have it to only collect a week worth of metrics.\n/opt/kafka 500MB for all of the topic data, couple gigs, nothing special.\n/opt/grafana 150MB to hold the configuration data, one key thing for this one is that the mount point has to be owned by a unique ID for it to work properly, something odd but to be on the lookout for.\n/opt/media 250MB for all of the configurations created by the containers cause I didn‚Äôt want to setup APIs every time i needed those services.\n/opt/mysql 500MB to hold all of my local databases.\n\nNetworks\nThe final step was to figure out a way for some of these to talk to each other, was going to use NGINX but kept seeing ‚ÄúTraefik‚Äù in Reddit so I figured I‚Äôd give it a shot‚Ä¶. after couple hours of trying and reading, got it to work in a way that makes sense to me and best of all I could integrate it perfectly with organizr.\nTraefik requires you to run all of the containers you want to route the traffic to be on the same network, which is easy with docker.\ndocker network create traefik-proxy --driver overlay\n\nWith that setup, all you have to do is define the network in your compose files.\nCompose Files\nSince I have a bunch of containers and to give you an idea I‚Äôm only going to show you couple examples, if you would like to see all configs feel free to reach out on social media (details at the bottom).\nAs a reminder, I got 3 Nodes in Docker Swarm mode with node labels so that I can force docker to run the containers on specific nodes based on the hardware requirements behind them.\n$ docker node inspect manjaro-server | jq .[0].Spec.Labels.name\n&quot;server&quot;\n$ docker node inspect manjaro-home | jq .[0].Spec.Labels.name\n&quot;home&quot;\n$ docker node inspect pi | jq .[0].Spec.Labels.name\n&quot;pi&quot;\n\nTraefik\nThe documentation suggests you run traefik on your master node so it can control and view all of the container events so I‚Äôve forced it to run on the manager node all the time.\nversion: &quot;3.7&quot;\nservices:\n  traefik:\n    image: traefik:v2.0\n    networks:\n      - traefik-proxy\n    command:\n      - --entrypoints.metrics.address=:8082\n      - --entrypoints.http.address=:80\n      - --api.dashboard=true\n      - --api.insecure=true\n      - --providers.docker\n      - --providers.docker.swarmMode=true\n      - --providers.docker.exposedByDefault=false\n    ports:\n      - 80:80\n      - 8080:8080\n      - 8082:8082\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    deploy:\n      placement:\n        constraints:\n          - node.role == manager\n\nnetworks:\n  traefik-proxy:\n    external: true\n\nGuacamole\nRunning this as well on the node that is 24/7 so i can access it remotely whenever i want.\nversion: &quot;3.7&quot;\nservices:\n  guacamole:\n    image: oznu/guacamole\n    volumes:\n      - /opt/guacamole:/config\n    networks:\n      - traefik-proxy\n    deploy:\n      placement:\n        constraints:\n          - node.labels.name == server\n      labels:\n        - &quot;traefik.enable=true&quot;\n        - &quot;traefik.http.routers.guacamole.entrypoints=http&quot;\n        - &quot;traefik.http.routers.guacamole.rule=Host(`guacamole.local.net`)&quot;\n        - &quot;traefik.http.services.guacamole.loadbalancer.server.port=8080&quot;\n        - &quot;traefik.docker.network=traefik-proxy&quot;\n\nnetworks:\n  traefik-proxy:\n    external: true\n\nConclusion\nWith a couple compose files pushed to a private GIT repo (mostly for sensitive data on the configurations, need to learn how to use docker secrets) my stack is fully automated, now with an ansible playbook I can have the same setup in minutes.\nAgain, this was a long task and I did learn a lot on how to debug and troubleshoot mostly networking problems with containers and being my first real project that used docker-compose it was good fun.\nI hope you liked it, until next time, Adios."},"posts/mentors-are-critical-in-your-career":{"title":"Mentors are critical in your career","links":[],"tags":["random"],"content":""},"posts/neovim":{"title":"Neovim in 2023","links":[],"tags":["neovim"],"content":"I‚Äôve been using Neovim entirely for everything that I can for over a year now and I gotta say I won‚Äôt go back to the regular old days of VScode.\nNow I‚Äôm one with Lua/Neovim and everyone else using the bloated editor is wrong (hot take üî•).\nHere‚Äôs why\n\n\nIt‚Äôs way way way faster than Vscode at loading things, resource wise it‚Äôs just fantastic, don‚Äôt have to wait couple seconds for a project to load up and my system to start choking cause of course Electron wants 20 out of the 32GB available.\nAnd this is not caused by something like running outdated hardware, top of the line 2019 MacbookPro can‚Äôt handle a big project with 20-30 files.\n\n\nCan be customized to whatever you mind can think of. Need your editor to do something like idk maybe toggle or fill out a [ ] in your todo files where you track everything? Write a quick 15 line function in lua.\nNeed to be able to run something with the code you are writing? There‚Äôs a probably a plugin for that and IF NOT you can write it.\n\n\nMakes you faster, once you go full VIM motions you can‚Äôt go back to hitting the arrows in your keyboard to find words or jump to places, you will simply become a better and faster developer by using the hot keys to get around and do things fast, like seriously fast.\n\n\nLSPs are fantastic, now here we have to give props to Micro$oft for making LSPs a thing, deep down Vscode uses that but by letting us have clients and servers run outside of the editor we can basically have the same functionality in neovim. Which also tends to be faster.\nWant to code in go? Setup the LSP. How about python? Same thing, just setup your neovim to use the lsp and the rest is history.\n\n\nKeybind madness, this could be a pro or con cause some people might not like to have 200 keybinds to do multiple things and also memorize them so you can use them, the best thing you can do is start small and use tools like Telescope to see which keybindings are set up in your environment.\nThere‚Äôs also plugins that give you visual clues as to what you set up like whichkey\n\n\nYou will always tweak your config, this is just a fact, once you understand how it works you will keep adding stuff to it. Welcome to the club.\n\n\nNot everything is peachy tho\n\n\nLearning curve, the first days or weeks you are gonna try to pull your hair out cause holy molly you keep trying to use the same arrow keys or the mouse to get around. Personally the use of hjkl took me couple weeks to get used to.\n\n\nFragmented, you can extend and write plugins as you please so guess what?. There‚Äôs like 25 plugins that do the same thing but with a different approach so finding a plugin tends to be hard cause theres so many that do the same thing.\n\n\nSetup is complex, one thing that Vscode does good is the actual setup of plugins. People can find things quick while in neovim you have to know how the rtpath works, how plugins are loaded, when are they loaded, etc.\nWhich makes this not exactly beginner-friendly.\n\n\nIf you endured and came out triumphant you will be happy you became a neovim zealot, you will look cool when your dev friends pair with you and you will feel like a freaking hacker.\nDon‚Äôt trust me?; Just watch either @teej or @theprimeagen code over at twitch and prove me wrong.\nMy personal setup looks like this:\n{% image src=‚Äúnvim.png‚Äù alt=‚ÄúNeovim Setup‚Äù /%}\nConclusion\nMaybe you will end up writing your own plugin for something that you wanted to do like maybe running terraform within your buffers/windows.\nTake a leap and give it a go, have some patience and join the cult.\nIf you want to check out how to make your config look like mine, check out my dotfiles here"},"posts/new-blog":{"title":"Blog 2.0","links":[],"tags":["random"],"content":"./Launch ‚Äîv=2.0\nSometime last year I was dead set on starting to blog but then life hit me hard in a positive way. My baby girl was born, got a new project in a different city so I had to move across the country‚Ä¶that was an adventure on it‚Äôs own. So between setting my life up again, getting to know people and learning all sorts of new things for my role I completely forgot about the blog not to mention that now that I‚Äôm a dad I love spending my free time with my baby girl.\nStill the blog was still in the back of my mind so for this 2019 instead of making it part of my ‚ÄòNew Year Resolution‚Äô which to be honest I never fullfil, decided to just YOLO do it. But this time it was going to be more ‚Äúme‚Äù if it makes sense and since I now know more on how Python/HTML/CSS/JS work I came up with a new theme and everything customized to what I wanted (some things are still pending like mobile navigation but I‚Äôll get to it, promise!).\nThere are a number of blog posts I will share with everyone and some of them include some of the new tools that I‚Äôm using at work so it should be fun."},"posts/nix-journey":{"title":"I Love/Hate Nix","links":[],"tags":["nix","wsl"],"content":"I‚Äôve always tried to automate my working environments in either a personal or professional setup. So years ago, I created a repository for my dotfiles which has been curated over the years with my custom applications, aliases or anything that I found useful or repetitive. All of that worked out just fine, at some point I even went all in into actual automation by using Ansible to set up my multiple machines, which also has its own repository if you are curious.\nLife was good, and it seems like people were using either Ansible or asdf (hell I even wrote a plugin for that  remember) to sort of automate their development setups, then Nix appeared with that damn smile.\n\nI‚Äôve heard of Nix before but never really did much with it cause it honestly looks super complicated to get started with, but as software engineers we like to suffer in silence. It all imploded when I saw one of my favorite streamers ALT-F4-LLC use Nix for basically everything, and he was kind enough to explain all of us, and he still does explain people whenever they ask on stream how it works and how to get started so with that motivation it was time to jump into the abyss of Nix.\nWhat the hell is nix anyway?\nWell it‚Äôs 3 things according to a lot of people, it‚Äôs called the nix trinity:\n\nSo with that in mind, I‚Äôm currently using 2 out of 3 of those components.\n\nThe Package Manager\nThe Nix Language\nThe operating system\n\nThe package manager helps me basically install anything that I would ever need on my system, it also lets me declare what my computer will need in a file so if for some reason this one blows up I can always just recover in seconds by using the configuration file, think of it like GitOps but for your personal system (this can also be applied or used on production machines that need to have certain pieces of software installed/enabled).\nThen the Nix Language which is the part that I was scared of the most is a way to declare functionality and let you configure your system in various ways. The 3rd part which is NixOS is where it all sort of comes together as you can basically create a new ‚Äúgeneration‚Äù of your machine by modifying and applying the latest version of your configuration file. The language itself isn‚Äôt exactly super complex, but it‚Äôs syntax for someone that me that isn‚Äôt used to something simpler than python/Golang looks a bit odd. But at the end of the day it has its little gotchas and once you go over that portion it‚Äôs a bumpy ride but a pleasant one (remember we like suffering).\nWhich takes me to one of the big first problems with Nix as a whole, and it‚Äôs something a lot of other people express as well.\n\nThe documentation is all over the place and there‚Äôs a million ways to do the same thing\n\nBut fear not, ChatGPT has been pretty good at telling me how to overcome certain obstacles while using Nix.\nShow me what I can do with it already\nAlright if you are sold on the idea, you can install Nix in your system by following the official documentation. You might want to start with the package manager which comes with the language portion.\nOnce it‚Äôs installed you can then start defining ‚Äúshells‚Äù or building ‚Äúpackages‚Äù. In all the examples we will be using nix flakes which are ‚Äúexperimental‚Äù but pretty reliable and help you reproduce things pretty nicely.\nSo make sure you enable that following the  documentation .\nThink of Flakes as the equivalent of using your package manager (npm, poetry, gem) that will grab dependencies and generate a lock file, so it knows what to use\nLet‚Äôs begin with an example of a shell that will have something like psql that is part of the postgresql package. Create a flake.nix file and include this.\n{\n  description = &quot;Loads PSQL&quot;;\n \n  inputs.nixpkgs.url = &quot;github:NixOS/nixpkgs/nixpkgs-unstable&quot;;\n    outputs = { nixpkgs, self}:\n      let\n        system = &quot;x86_64-linux&quot;;\n        pkgs = import nixpkgs { inherit system; config.allowUnfree = true;};\n      in {\n        devShells.${system}.default = pkgs.mkShell {\n          buildInputs = [\n            pkgs.postgresql\n          ];\n        };\n      };\n}\n\nThe file may look complex but in reality it‚Äôs just inputs and outputs declared.\n\nWith the file created now you can do nix develop. Nix will find your flake file and use it to build a shell environment.\nYou can now do psql and use it to connect to a database.\nThis is the power of Nix, having the option to run certain packages on certain projects where a flake.nix file resides. Now you can make that array bigger and start including whatever your project might need and do remember that the nixpkgs repository has more packages/apps than the AUR which is considered to be massive.\nAutomate your shells\nIf you are now somewhat sold, what if I told ya you won‚Äôt need to run nix develop and as soon as you change directory into something with a flake everything will be prepared and enabled for you to use?.\nThat where direnv comes in, it‚Äôs another project/tool that basically allows you to automagically load your environments as soon as it sees a flake.nix and a .envrc file.\nIf you want to set it up please follow the instructions\nHere‚Äôs an example on how it works.\n# before direnv\n‚ûú  psql\nThe program &#039;psql&#039; is not in your PATH. It is provided by several packages.\nYou can make it available in an ephemeral shell by typing one of the following:\n  nix-shell -p postgresql\n \n# after direnv\ndirenv: loading ~/git/example/.envrc\ndirenv: using flake\ndirenv: nix-direnv: using cached dev shell\ndirenv: export +AR +AS +CC +CONFIG_SHELL +CXX +HOST_PATH +IN_NIX_SHELL +LD +NIX_BINTOOLS +NIX_BINTOOLS_WRAPPER_TARGET_HOST_x86_64_unknown_linux_gnu +NIX_BUILD_CORES +NIX_CC +NIX_CC_WRAPPER_TARGET_HOST_x86_64_unknown_linux_gnu +NIX_CFLAGS_COMPILE +NIX_ENFORCE_NO_NATIVE +NIX_HARDENING_ENABLE +NIX_LDFLAGS +NIX_STORE +NM +OBJCOPY +OBJDUMP +RANLIB +READELF +SIZE +SOURCE_DATE_EPOCH +STRINGS +STRIP +__structuredAttrs +buildInputs +buildPhase +builder +cmakeFlags +configureFlags +depsBuildBuild +depsBuildBuildPropagated +depsBuildTarget +depsBuildTargetPropagated +depsHostHost +depsHostHostPropagated +depsTargetTarget +depsTargetTargetPropagated +doCheck +doInstallCheck +dontAddDisableDepTrack +mesonFlags +name +nativeBuildInputs +out +outputs +patches +phases +preferLocalBuild +propagatedBuildInputs +propagatedNativeBuildInputs +shell +shellHook +stdenv +strictDeps +system ~PATH ~XDG_DATA_DIR\n‚ûú psql --version\npsql (PostgreSQL) 15.6\n \nI find that pretty cool honestly. Loading everything as soon as you enter a folder/project?\n\n## How can I use it to build things?\nIf development environments were not enough to impact you, then let&#039;s see how we can sort of replace Makefile with nix too.\nLet‚Äôs pretend you have a Go app, and you want to build or run it without using go build or go run which can receive more parameters to build it for certain architecture or find your package somewhere where your main package resides.\nMaybe also use Nix to build the docker application.\nThe function/packages mkShell used in the previous example, is one of many things you can leverage. So let‚Äôs use some other functions buildGoModule.\n{\n  description = &quot;A very basic flake&quot;;\n  inputs = {\n    nixpkgs.url = &quot;github:nixos/nixpkgs?ref=nixos-unstable&quot;;\n  };\n  outputs = { self, nixpkgs }:\n    let\n      system = &quot;x86_64-linux&quot;;\n      pkgs = import nixpkgs { inherit system; };\n      name = &quot;your-project-name&quot;;\n      vendorHash = &quot;yourProjectHash&quot;;\n    in\n    {\n      devShells.${system}.default = pkgs.mkShell {\n        inputsFrom = [ self.packages.${system}.default ];\n        nativeBuildInputs = [ pkgs.air pkgs.templ pkgs.sqlite ];\n      };\n      packages.${system}.default = pkgs.buildGoModule {\n        inherit name vendorHash;\n        src = ./.;\n      };\n    };\n}\n \nNow doing something like nix run or nix build will allow you to have either the binary built or can even be expanded to do things like building a docker image you can then export/load/push onto a registry.\nThe trick in the flake above is the use of module buildGoModule which basically knows how to build go packages. Similar to this function, you will find them for different languages. Or you can always go raw and use mkDerivation which lets you control what to run and what to inject into the sandbox nix creates while building your project.\nNote: This was a very quick intro guide on some of the power of nix + flakes so if you would like to see more, or ask questions feel free to jump on stream and ask away!.\nConclusion\nThe more I play with Nix the more I Hate/Love it because:\n\nEverything can be declared and reproduced\nThe language is sort of a pain to work with sometimes (Functional programming type of deal) and also the syntax is weird in my opinion.\nDocumentation is scarce, so sometimes ChatGPT is your best bet\nNix can do pretty much anything in a computer\nNixOS can do even more cause it lets you spin up services, packages or whatever you need\nYou can install and find pretty much anything in the repository\nAllows me to say, I use Nix btw.\n\nIf you would like to learn more there are excellent tutorials and blog posts out there, some of the stuff that helped me:\n\nBuilding a rust service with nix\nGo Programs Nix\nNixOS and Flakes Book\nNix Wiki\n\nHope you like it.\nJoin the nix cult!\nAdios üëã"},"posts/ollama-wsl-nixos":{"title":"Ollama in WSL + NixOS","links":[],"tags":["wsl","nix"],"content":"While setting up my new computer and WSL + NixOS (btw), I needed to set up my own quick LLM to ask dumb things and integrate it within my NeoVim experience using the Ollama Plugin.\nSo here‚Äôs the quick guide on how to get that done since it can be tricky to decide things like:\n\nShould Ollama be installed within my WSL host?\nSpecially in NixOS do I need to set up some weird service?\nDo I maybe install it directly on Windows?\nWhy is my life so complicated as a Nix user?\n\nSo for whoever is reading this and wants to get the setup done quick here‚Äôs what I did after struggling with all the above questions. The last one still hurts.\n\nDo not install the NixOS app, it seems like it just gets you the CLI wrapper, but there‚Äôs no service or anything running.\nYou might think well I‚Äôm going to install the service as well, don‚Äôt, it takes ages to compile and run and it will not use your GPU. The cli won‚Äôt even recognize the service so WTH?.\nInstall Ollama on your windows host directly by following this and proceed to install the models via Powershell or your personal terminal emulator.\n\nIf you want to make this work in your own emulator make sure the ollama.exe is added to your path\nIn NixOS you can add all of the windows PATH to your WSL by enabling   wsl.interop.includePath = true;\n\n\nWith Ollama and some models you will still not be able to hit the API, so you need to tell the service to run on 0.0.0.0. NOTE: By default it‚Äôs just listening on 127.0.0.1 = you cannot hit it from within WSL.\nSet up an Environment Variable in Windows OLLAMA_HOST=0.0.0.0\nRestart the Ollama service (Kill the app in the tray icon and start it over)\nValidate you can hit the API nc -zv &lt;ip-of-your-window-host&gt; 11434. You can get the IP by running ifconfig in your Windows host.\nProfit!\n\nThere are currently couple issues in Ollama‚Äôs Github tracker related to the fact that not all interfaces can hit the services so hopefully down the road it becomes a setting we can do via UI or a service definition. Meanwhile an environment variable will suffice.\nIf you did everything right you should now be able to connect to the API from your editor of choice and start using the benefits.\nI know there are some Obisidian Plugins as well that can leverage Ollama for completion or better notes, so maybe that‚Äôs something I will explore next!.\nHope this quickie one helps you!\nAdios üëã"},"posts/pihole-is-awesome":{"title":"Pihole is pure awesomeness","links":["[https:/firebog.net/](https:/firebog.net/)"],"tags":["homelab"],"content":"Its been months ever since I setup my pihole and I never noticed how much it helps reduce the number of ads and spam I used to see in the webpages until one day the SD card on my raspberry pi was at 100% so everything running in there stopped responding and I never noticed it‚Ä¶ dont be like me and monitor your devices, anyways I started seeing a bunch of pop ups, modals and ads everywhere‚Ä¶ That‚Äôs when I realized once again how much everyone needs a pihole in their Network and also how infected the internet is‚Ä¶ of just so much random stuff that tracks you and serves trash.\nSo figured since I ended up adding a second pihole to my network as a contingency in case the main one runs into issues again I should tell you how awesome the pihole is and why you need one.\nThe concept of a pihole is quite simple, to block all DNS requests to known malicious and ad provider endpoints, giving you virtually an ad free and safe browsing experience. This is accomplished by turning a device (machine/vm/container) into a DNS server and pointing all your devices to use it as an upstream.\nOne thing to note is that there are certain sites like YouTube that basically inject the ad into the video feed so blocking it might effectively block you from watching videos so do keep that in mind if your sole purpose was to stop YouTube from serving you ads, this won‚Äôt work. A shame but FYI.\nAnother fantastic thing of this project is that the ad lists are maintained by the community so if you notice that a device of yours is doing some extraneous calls and block it you can contribute that domain for everyone to benefit. Trust me once you have a pihole running and you review your traffic from your devices you will notice how much certain things like smart TVs and WiFi cameras make requests every couple minutes or seconds to weird domains.\nEasier to show you the benefits, like a good ol‚Äô before and after type of deal‚Ä¶\n\nAaaaaaand they‚Äôre gone.\n\nSo you are convinced how do you get started, you ask?. Well following the installation depending on the device you want to run it on.\n\nLazy one line installer\n\ncurl -sSL install.pi-hole.net | bash\n\nVia docker-compose, in my case I do not use my pihole as DHCP but you can do that if you want to.\n\nversion: &quot;3&quot;\n \n# More info at github.com/pi-hole/docker-pi-hole/ and docs.pi-hole.net/\nservices:\n  pihole:\n    container_name: pihole\n    image: pihole/pihole:latest\n    ports:\n      - &quot;53:53/tcp&quot;\n      - &quot;53:53/udp&quot;\n      - &quot;67:67/udp&quot;\n      - &quot;80:80/tcp&quot;\n      - &quot;443:443/tcp&quot;\n    environment:\n      TZ: &#039;America/Chicago&#039;\n      # WEBPASSWORD: &#039;set a secure password here or it will be random&#039;\n    # Volumes store your data between container upgrades\n    volumes:\n      - &#039;./etc-pihole/:/etc/pihole/&#039;\n      - &#039;./etc-dnsmasq.d/:/etc/dnsmasq.d/&#039;PN\n    # Recommended but not required (DHCP needs NET_ADMIN)\n    #   github.com/pi-hole/docker-pi-hole#note-on-capabilities\n    cap_add:\n      - NET_ADMIN\n    restart: unless-stopped\nWith the package or container running, you can visit the web interface at port 80, you should see something like this.\n\nNow its time to send all of your DNS traffic to the pihole, the easiest way is to tell your router to send all traffic to the IP of the machine/device running the service but I‚Äôve found that most of the times the actual computers in your network might not want to really send the traffic via pihole so its easier to force them.\nIf you have couple UNIX machines simply modify your /etc/resolv/conf and add the nameserver\n# /etc/resolv.conf\nnameserver &lt;ip-device-or-container&gt;\nOn windows you have to do it using the ipv4 settings in the network panel as shown below.\n\nIf everything is done right you should see something similar to mine that indicates that most of the devices are sending traffic to the PiHole, a quick way to ensure it‚Äôs really working is to visit any site you want and see if that domain appears in your log. So if I visit my own blog I‚Äôd end up seeing this.\n\nThe final and fun part is adding domains you want to block to your PiHole, so inside your admin panel go to Group Management &gt; Adlists and start adding away, one great place to get lists is reddit or this site. Do not forget to update your settings after adding some lists by going to Tools &gt; Update Gravity which effectively reads those Adlists and adds them to your local PiHole. It is recommended you update your lists frequently in case the owners or maintainers add more stuff to it. I‚Äôm quite happy with mine that contains 860K domains and I rarely see issues while loading pages with those.\nBONUS : If you ever want to mess around with your family go ahead and block facebook and instagram, let me know how that played out for you.\nAnyways, hope this all made sense, if not please feel free to tell me why this sucked on my social media that you can find down below. See ya next time!"},"posts/pushgateway-pihole-speedtest":{"title":"Pihole and ISP Metrics in Prometheus","links":[],"tags":["automation","homelab"],"content":"In a previous post I show cased how you could start collecting some of the metrics that scripts like speedtest can dump out and you can leverage that data by ingesting into an Elasticseach cluster so you can later visualize in Kibana dashboards and while that is super nice, the more and more I keep playing with Time Series Databases the more I start to think that full blown logging systems like Elastic and Splunk are simply an overkill for simple numerical metrics like values at specific points in time. Not to stay the tools can‚Äôt handle it but its like using a jackhammer to hang a picture in your wall.\nOut of all the TSDBs I‚Äôve played with, the one I enjoy the most is Prometheus. It‚Äôs simple to use, light weight, can be run in a container and it has a lot of mechanisms to push data into it. Right now my current instance is collecting metrics from my docker daemon runnning all of the containers in swarm mode, my system metrics from 2 computers and my reliable Raspberry Pi and with a Grafana instance on top of it I have visibility on everything my small homelab might need.\nThe ‚ÄúProblem‚Äù\nYou might ask, well why move what is already working on Elastic to a new system?. Well I was bored and also had the need to add some custom ‚Äúmetrics‚Äù I wanted to keep track off and wanted to use an actual TSDB for some actual metrics so here‚Äôs the problems im trying to solve. Adding these to something like Elastic would‚Äôve been easy as well but the need to try something new won me over. Here‚Äôs what I intend to add.\n\nMy Piholes for some reason stop responding to DNS queries after some point, I need to sit down and figure out why but meanwhile doing a restartdns works, so it‚Äôs scheduled in a cron job but I would like to know the number of resets i‚Äôve done on each.\nThe number of clients on each Pihole tends to change over time so I would like to visualize which one is taking more traffic.\nThe number of block elements on each pihole should be in sync but since there‚Äôs no clustering availble yet, I have to manually keep them alligned so if the number of block pages is not the same on each I can easily see it in a graph.\nFinally, move all of the ISP metrics I was collecting before and pushing into elastic will now go to Prometheus (download/upload speed, lattency, etc.)\n\nThe ‚ÄúSolution‚Äù\nOne of the downsides is that Prometheus doesn‚Äôt directly let you POST data into it with something like curl cause it works on a ‚Äúpull model‚Äù meaning it only reads from external, it doesnt actually receive anything from the ouside unlike other tools like ES or Splunk, you could potentially use a client library to collect said metrics and generate an endpoint Prometheus can read‚Ä¶ but I am not that bored‚Ä¶ so we have to leverage a separate component called ‚ÄúPushGateway‚Äù that will basically work as a sink to collect everything you push into it and then your Prometheus instance will scrape all of the metrics it finds and store them into the TSDB. In my case since everything I run is ‚Äúdockerized‚Äù I will use the container version of it.\nThe full documentation on how to instrument and push data into it can be found here github.com/prometheus/pushgateway\nSo let‚Äôs start the gateway.\ndocker run -d -p 9091:9091 prom/pushgateway\nWith that listening you can navigate to http://localhost:9091 and you should see a basic UI.\nWhen you push the data you have to be mindful on how you name it and what other properties you pass to it like instance or labels if applicable.\nAll of your metrics must follow the pattern url:port/metrics/job/&lt;job_name&gt;/instance/&lt;instance_name&gt;\nNow we will do a very simple sets of bash scripts that will be running via cron or systemd timers to basically collect data and then push those metrics into the gateway. First one will restart the DNS service and increase the counter.\n#!/bin/sh\n# Set variables\nJOB_NAME=pihole\nINSTANCE_NAME=pi\n \n# Execute Action\npihole restartdns\n \n# Post to Gateway\ncat &lt;&lt;EOF | curl --data-binary @- http://localhost:9091/metrics/job/$JOB_NAME/instance/$INSTANCE_NAME\n  pihole_reset_counter 1\nEOF\nTo Grab the number of current blocked domains in each pihole, we could interact with the sqlite3 database that ships with the tool and we will simply query the gravity table that holds all domains and export a base count, save that on a variable and push to the gateway‚Ä¶but since the tool also comes with a nice way to export a lot of the good stats for people that plug in LCDs to their Raspberry Pi we will explode that‚Ä¶who wants to do SQL anyway‚Ä¶ sqlite3 /etc/pihole/gravity.db &quot;select count(*) from gravity&quot;. For ‚Äúparsing‚Äù the data exported in JSON we will also use jq.\n#!/bin/sh\n# Set variables\nJOB_NAME=pihole\nINSTANCE_NAME=pi\n \n# Execute Action\nPIHOLE_STATS=$(pihole -c -j)\nPIHOLE_DOMAINS_BLOCKED=$(echo $PIHOLE_STATS | jq .domains_being_blocked)\nPIHOLE_DNS_QUERIES=$(echo $PIHOLE_STATS | jq .dns_queries_today)\nPIHOLE_BLOCKED_QUERIES=$(echo $PIHOLE_STATS | jq .ads_blocked_today)\n \n# Post to Gateway\ncat &lt;&lt;EOF | curl --data-binary @- http://localhost:9091/metrics/job/$JOB_NAME/instance/$INSTANCE_NAME\n    pihole_blocked_domains $PIHOLE_DOMAINS_BLOCKED\n    pihole_dns_queries $PIHOLE_DNS_QUERIES\n    pihole_blocked_queries $PIHOLE_BLOCKED_QUERIES\nEOF\nFinally for the ISP metrics I will reuse the speedtest-cli to output the data needed in JSON and parse it with jq. Quite simple right?.\n#!/bin/sh\n# Set variables\nJOB_NAME=speedtest\nINSTANCE_NAME=pi\n \n# Execute Action\nSPEEDTEST_DATA=$(speedtest --json --single)\nSPEEDTEST_PING=$(echo $SPEEDTEST_DATA | jq .ping)\nSPEEDTEST_LATENCY=$(echo $SPEEDTEST_DATA | jq .server.latency)\nSPEEDTEST_UPLOAD=$(echo $SPEEDTEST_DATA | jq .download)\nSPEEDTEST_DOWNLOAD=$(echo $SPEEDTEST_DATA | jq .upload)\n \n# Post to Gateway\ncat &lt;&lt;EOF | curl --data-binary @- http://localhost:9091/metrics/job/$JOB_NAME/instance/$INSTANCE_NAME\n    speedtest_ping $SPEEDTEST_PING\n    speedtest_latency $SPEEDTEST_LATENCY\n    speedtest_upload $SPEEDTEST_UPLOAD\n    speedtest_download $SPEEDTEST_DOWNLOAD\nEOF\nWhen these scripts are executed manually or by systemd/cron you should now see your metrics show up in the pushgateway UI. All that is left is to configure your Prometheus instance to scrape the Pushgateway‚Ä¶ if you are curious you can see the metrics in http://localhost:9091/metrics.\nHere‚Äôs how mine look for the one test host‚Ä¶the idea is to copy the same set of scripts to my machines running pihole and just modify the variables as needed to use a different instance name.\n\nNOTE: The pushgateway is ideal for short dumb things like this, in a PROD environment you might want to consider using something else since this becomes a single point of failure as the documentation says so only use it for fooling around like me or consult with some professionals with more experience using it.\nConclusion\nPrometheus is a great TSDB and it‚Äôs super simple to run, quite popular and the default time series database for big projects like Kubernetes‚Ä¶ not to mention it‚Äôs a ‚Äúgraduated‚Äù project from the CNCF. You can pair it with something like Grafana and you can go crazy with the amount of things you can create. Again, for PROD usage you might want to have multiple instances and in federated mode for that High Availability, but since these posts are mostly me playing around with tech you can replicate the single node point of failure model and it will work great until it doesn‚Äôt.\nI do wish that Prometheus would let you push data in directly but that completely breaks the pull model so we will have to live with the pushgateway and the multiple client libraries that emulate a mini webserver that your Prometheus instance can scrape.\nAt the end of this I‚Äôve tackled all of the ‚Äúproblems‚Äù I created for myself and the next step is getting those cool dashboards‚Ä¶On my next spring of boredoom I will generate those in Grafana and share them in the post."},"posts/python-elastic":{"title":"Twitter Sentiment Analysis with Python & Elasticsearch","links":[],"tags":["automation","elk"],"content":"Elasticsearch has become part of my daily routine so the more I use it, the more I think of ways of using it outside work so came up with the idea of why not creating my own ingestion with sentiment analysis so that data can be processed and tagged before being indexed into Elastic?.\nI know Logstash has already a plugin to ingest data from twitter but since i also wanted to add a bit of polarity to each tweet and also wanted to control the process since I truly don‚Äôt want to ingest a lot of data as I don‚Äôt have unlimited storage so i decided to make my own and turns out it was quite simple.\nNow to being, the dependencies I used for this were:\n\nElasticsearch 6.5\npython-elasticsearch\ntwython\ntextblob\n\nElastic offers 2 libraries to interact with your node, so make sure you pip install this one.\nStart your ES instance\nNow setting an instance could be complicated so i‚Äôll just go over some very basic setup, if you want something more ellaborate the elastic.co documentation is quite good.\n\nMake sure you have java installed.\n\njava --version\nopenjdk version &quot;1.8.0_192&quot;\nOpenJDK Runtime Environment (build 1.8.0_192-b26)\nOpenJDK 64-Bit Server VM (build 25.192-b26, mixed mode)\n\nDownload Elasticsearch from here. This will be different based on your OS/Distro. Again in my case I went with 6.5 since I run ‚ÄúLinux-Manjaro‚Äù.\nExtract the contents.\nLocate and run the binary, it‚Äôs usually located inside elasticsearch/bin/elasticsearch. The process should start and you should see something like this.\n\n[2018-12-24T07:52:53,670][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [aggs-matrix-stats]\n[2018-12-24T07:52:53,670][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [analysis-common]\n[2018-12-24T07:52:53,671][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [ingest-common]\n[2018-12-24T07:52:53,671][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [lang-expression]\n[2018-12-24T07:52:53,671][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [lang-mustache]\n[2018-12-24T07:52:53,671][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [lang-painless]\n[2018-12-24T07:52:53,671][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [mapper-extras]\n[2018-12-24T07:52:53,671][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [parent-join]\n[2018-12-24T07:52:53,671][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [percolator]\n[2018-12-24T07:52:53,671][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [rank-eval]\n[2018-12-24T07:52:53,671][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [reindex]\n[2018-12-24T07:52:53,671][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [repository-url]\n[2018-12-24T07:52:53,671][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [transport-netty4]\n[2018-12-24T07:52:53,671][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] loaded module [tribe]\n[2018-12-24T07:52:53,672][INFO ][o.e.p.PluginsService     ] [YmQ2k-V] no plugins loaded\n[2018-12-24T07:52:57,413][INFO ][o.e.d.DiscoveryModule    ] [YmQ2k-V] using discovery type [zen] and host providers [settings]\n[2018-12-24T07:52:58,116][INFO ][o.e.n.Node               ] [YmQ2k-V] initialized\n[2018-12-24T07:52:58,116][INFO ][o.e.n.Node               ] [YmQ2k-V] starting ...\n[2018-12-24T07:52:58,562][INFO ][o.e.t.TransportService   ] [YmQ2k-V] publish_address {127.0.0.1:9300}, bound_addresses {[::1]:9300}, {127.\n0.0.1:9300}\n[2018-12-24T07:53:01,689][INFO ][o.e.c.s.MasterService    ] [YmQ2k-V] zen-disco-elected-as-master ([0] nodes joined), reason: new_master {Y\nmQ2k-V}{YmQ2k-VPQKGmDK_xcRSQuQ}{yKFFqQ0xQHGmXjNxu89gAQ}{127.0.0.1}{127.0.0.1:9300}\n[2018-12-24T07:53:01,696][INFO ][o.e.c.s.ClusterApplierService] [YmQ2k-V] new_master {YmQ2k-V}{YmQ2k-VPQKGmDK_xcRSQuQ}{yKFFqQ0xQHGmXjNxu89g\nAQ}{127.0.0.1}{127.0.0.1:9300}, reason: apply cluster state (from master [master {YmQ2k-V}{YmQ2k-VPQKGmDK_xcRSQuQ}{yKFFqQ0xQHGmXjNxu89gAQ}{\n127.0.0.1}{127.0.0.1:9300} committed version [1] source [zen-disco-elected-as-master ([0] nodes joined)]])\n[2018-12-24T07:53:01,714][INFO ][o.e.h.n.Netty4HttpServerTransport] [YmQ2k-V] publish_address {127.0.0.1:9200}, bound_addresses {[::1]:9200\n}, {127.0.0.1:9200}\n[2018-12-24T07:53:01,715][INFO ][o.e.n.Node] [YmQ2k-V] started\nNOTE: If you want to run it in the background add parameters -d to daemonize it.\nFinally test to see if your node is ready by performing a request against your localhost in port 9200 which is the default used by ElasticSearch. In my case I named my node ‚Äúnode-1‚Äù and my cluster ‚Äúhome-cluster‚Äù\ncurl localhost:9200\n{\n&quot;name&quot; : &quot;node-1&quot;,\n&quot;cluster_name&quot; : &quot;home-cluster&quot;,\n&quot;cluster_uuid&quot; : &quot;Ma_eYy0UT1C5b0WwOhQshw&quot;,\n&quot;version&quot; : {\n  &quot;number&quot; : &quot;6.5.4&quot;,\n  &quot;build_flavor&quot; : &quot;default&quot;,\n  &quot;build_type&quot; : &quot;tar&quot;,\n  &quot;build_hash&quot; : &quot;d2ef93d&quot;,\n  &quot;build_date&quot; : &quot;2018-12-17T21:17:40.758843Z&quot;,\n  &quot;build_snapshot&quot; : false,\n  &quot;lucene_version&quot; : &quot;7.5.0&quot;,\n  &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;,\n  &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot;\n},\n  &quot;tagline&quot; : &quot;You Know, for Search&quot;\n}\n\nOk so now you have your single node cluster set, next step would be to create a ‚Äúmodel‚Äù for the data you will ingest, again since i don‚Äôt have unlimited storage or more nodes I will tweak the mapping for all of the indices that get created to just have 1 shard with no replicas. This is an elasticsearch type of deal so if you want to learn more, i would again point you to the documentation or you can ask me (social media stuff at the bottom).\n\nNow i could create the mapping everything i index the data but then again, that‚Äôs manual stuff which i kind of despise so i went ahead and created a template so that all indices that would match the pattern would adopt the settings.\n &quot;trump_tweets&quot; : {\n  &quot;order&quot; : 0,\n  &quot;index_patterns&quot; : [\n  &quot;trump-*&quot;\n  ],\n  &quot;settings&quot; : {\n  &quot;index&quot; : {\n    &quot;number_of_shards&quot; : &quot;1&quot;,\n    &quot;number_of_replicas&quot; : &quot;0&quot;\n  }\n  },\n  &quot;mappings&quot; : { },\n  &quot;aliases&quot; : {\n  &quot;trump-data&quot; : { }\n  }\n}\nSo once you have the mapping defined we are finally ready to push some data using Python!.\nIngesting data with python-elasticsearch\nAlright so the first thing we have to do is acquire some twitter credentials and token so that we can make use of the libraries to retrieve tweets, to get those credentials go here.\nFirst thin is to define the connection object that we will use to interact with Elasticsearch, also we will import the whole thing, since we are doing sentiment analysis we of course need those libraries.\nIn the last portion we tell elasticsearch that if the index called ‚Äòtrump‚Äô does not exist\nfrom textblob import TextBlob\nfrom elasticsearch import Elasticsearch\nimport uuid\nimport json\nfrom datetime import datetime\n \n# Elastic Connection\nes = Elasticsearch(hosts=&quot;localhost&quot;)\nindex_name = &#039;trump-&#039; + datetime.now().strftime(&#039;%Y.%m.%d&#039;)\nNext, we will define the data model used to describe each ‚Äòtweet‚Äô or event and pass it down to elasticsearch, in here is where we do the sentiment analysis using library ‚ÄòTextBlob‚Äô.\nclass Tweet(object):\n  def __init__(self, username, realname, location, tweet_text, hashtags):\n    self.id = str(uuid.uuid4())\n    self.timestamp = datetime.utcnow()\n    self.username = username\n    self.realname = realname\n    self.location = location\n    self.tweet_text = tweet_text\n    self.hashtags = [hash[&quot;text&quot;] for hash in hashtags]\n    self.sentiment = self.get_sentiment()\n \n  def get_sentiment(self):\n    return TextBlob(self.tweet_text).sentiment.polarity\n \n  def push_to_elastic(self):\n    es.index(\n      index=index_name,\n      doc_type=&quot;tweets&quot;,\n      id=self.id,\n      body={\n          &quot;@timestamp&quot;: self.timestamp,\n          &quot;user&quot;: self.username,\n          &quot;realname&quot;: self.realname,\n          &quot;location&quot;: self.location,\n          &quot;tweet&quot;: self.tweet_text,\n          &quot;hashtags&quot;: self.hashtags,\n          &quot;sentiment&quot;: self.sentiment,\n        }\n    )\n  def get_details(self):\n    print(self.timestamp, self.username, self.tweet_text, self.hashtags, self.sentiment)\nFinally we will make use of the client and data objects to start a stream that will push all of the tweets with our added data to the Elasticsearch index so that we can later do some searches and visualizations with it using Kibana.\nfrom twython import TwythonStreamer\nfrom models import Tweet, es\nfrom datetime import datetime\n \nCONSUMER_KEY = &quot;YOURKEYGOESHERE&quot;\nCONSUMER_SECRET = &quot;YOURKEYGOESHERE&quot;\nAUTH_TOKEN = &quot;YOURKEYGOESHERE&quot;\nAUTH_SECRET = &quot;YOURKEYGOESHERE&quot;\n \n \nclass MyStreamer(TwythonStreamer):\n  def on_success(self, data):\n    try:\n      tweets = Tweet(\n        username=data[&quot;user&quot;][&quot;screen_name&quot;],\n        realname=data[&quot;user&quot;][&quot;name&quot;],\n        location=data[&quot;user&quot;][&quot;location&quot;],\n        tweet_text=data[&quot;text&quot;],\n        hashtags=data[&quot;entities&quot;][&quot;hashtags&quot;],\n      )\n      tweets.push_to_elastic()\n    except KeyError:\n      pass\n \n \n  def on_error(self, status_code, data):\n    print(status_code)\n    self.disconnect()\n    return False\n \n  def on_timeout(self, data):\n    print(&quot;Request timed out, try again later&quot;)\n    self.disconnect()\n \ndef start():\n  stream = MyStreamer(CONSUMER_KEY, CONSUMER_SECRET, AUTH_TOKEN, AUTH_SECRET)\n  stream.statuses.filter(track=[&quot;Trump&quot;, &quot;trump&quot;])\n \n \nif __name__ == &quot;__main__&quot;:\n  start()\nNow that we have everything ready we can simply run the script and this should start pushing data to our single node cluster.\nTo validate, you can hit the endpoint ‚Äòhttp://localhost:9200/\\_cat/indices?v‚Äô and you should get something like.\nhealth status index                              uuid                   pri rep docs.count docs.deleted store.size pri.store.size\ngreen  open   trump-2019.03.09                   yYHYloR5TEGlenfKjYe4PQ   1   0     139190            0       59mb           59mb\nIn the next part we will start playing around with the data. If you have any questions, hit me up on social media."},"posts/self-hosting-engineer":{"title":"Why Self-Hosting made me a better engineer","links":[],"tags":["random"],"content":"It is a fact that people who are passionate about a topic or subject tend to know more or do better at said topics, so in the case of Software Engineers you probably have run into couple different types of engineers, which in my humble opinion (based on working in the industry for 12 years now); These are the 3 big ‚Äútypes‚Äù:\n\nThe guy who studied CS since it paid good and just work the 9-5. Extra hours probably spent touching grass.\nThe guy who went to CS because he just loves and is passionate about code and everything around it. Most likely works and maintains some open source projects on their downtime. Spends his time arguing over a language or framework on X or Twitter.\nThe self-taught who ended up in tech and comes from an unrelated tech background (nurse, musician, etc.) and is obsessed with it and somehow turns his entire personality and life into tech. Some are crazy enough that build an entire data center in the basement or closet.\n\nThat being said I‚Äôd consider myself type 3 as I never did CS at school and everything I know comes from experience and passion.\nSome of my big learning experiences that make me stand out vs other engineers at work are the following:\nNOTE: This is of course very Ops based, since that‚Äôs where I started my career in tech.\n\nHad to learn an amazing amount of information in order to even get the Homelab usable, from installing the OS to setting up the additional drives and mount points‚Ä¶ Setting up a secure SSH configuration, users and credentials, permissions on files, etc.\nSetting up the entire Networking, this one was huge for me as it helped me a TON to grasps concepts of DNS, TCP, iptables, ports/firewalls, etc. This one skill has made me surpass a lot of my fellow engineers at work, understanding how things are connected and work just changes your perspective. Suddenly everything clicks in your head.\nInstalling things, which at first were done manually but once you start adding more machines or services the manual steps become boring, repetitive and error-prone, we are humans, so we tend to screw those up. So tools like Ansible or Chef stated to appeal to me, so my machines could have the exact same configuration over and over.\nContainers disrupted my entire way of thinking and running things, so I learned how Docker works and how I could customize my own images and services. This got worse when my Homelab expanded, so I needed to run more containers in the machines so my good ol‚Äô network knowledge was useful at understand how those bridges were formed between nodes.\nEventually the hot thing was Kubernetes, the migration was painful and full of new concepts and way of doing things, but I feel comfortable now transforming any docker-compose with whatever number of services and containers into Kubernetes manifests.\nWith the complexity of the manifests tools like Helm Charts or Kustomize became another skill I had to learn, this one was somewhat easy since its just abstractions over manifests.\nStorage became important so a NAS was acquired and with my knowledge it was easy to set up and start generating some Volume Claims for my own needs.\nNetwork went from a basic Netgear router to a full blow installation consisting of:\n\nUnifi Dream Machine\nUnifi PoE Switches\nUnifi Wi-Fi Access Points\nThese devices introduced me into the work of proper networking where you have control over DNS, VLANs, Firewall Policies and actual useful telemetry of what is going on in the network. This part was also the most expensive one so far as those devices aren‚Äôt exactly cheap.\n\n\nAs my workloads became part of the entire family routine without them knowing, things needed to become accessible from in and outside our home. This is when things like Tailscale and Cloudflare tunnels became important.\nEverything that I paid for became a service I could self-host within my lab so the more things I ran, having availability and good monitoring became critical. To point out it‚Äôs what I do at work so, this became an excellent playground for me to test different technologies without causing outages.\n\nThis is probably the main reason why this whole thing started, I wanted a safe environment to test and break without disrupting anyone but myself.\n\n\nHaving the playground enabled me to learn how things are built; I started to learn coding and doing the usual dumb apps, turning them into containers and finally deploying them. Great way to see the full deployment lifecycle, nowadays people rely on other engineers or services to do this which is OK, not everyone needs to learn the tech and master it, but it‚Äôs somewhat important to know what‚Äôs going on behind the scenes IMO.\n\nAgain these are very Ops based which is funny because I built this entire thing to allow myself to learn to code and deploy somewhere end to end, Devops some people call it. That was the original motivation, not building and running on my local machine something that I could rely on or use outside my personal computer, from a little Raspberry Pi running a dumb container and a DNS server to an entire rack with couple machines and enterprise grade network devices.\nNow this is an overkill and probably steered into a non so coding arc, but I‚Äôve found that is what I‚Äôm passionate about, systems and how to set them up, figuring out how to configure software on them and make it reliable and easy to monitor effectively.\nHopefully this helps someone realize that not everything in Software Engineering is coding and being on top of the latest language or framework. Without the infra and people running it even on cloud it all becomes pointless.\nYes even those serverless functions require servers my dudes LOL.\nSee you on the next one.\nAdios üëã"},"posts/selfhosted-2022":{"title":"My SelfHosted Journey in [2022]","links":[],"tags":["homelab"],"content":"Back in 2020 I started the beautiful journey of running services in my gaming rig, a Dell SFF computer and a Raspberry Pi 3b and due to the pandemic I had plenty of time at home to start playing with tech that I described back in this post but much has changed since then, so I figured I would share where that journey has taken me.\nAt the begging I was only fooling around with services or platforms that I used at work cause I wanted to learn more but at some point the entire focus shifted when I started listening to the self-hosted podcast as well as visiting the r/self-hosted subreddit, boy was I amazed by the amount of things you could host in your own network and the amount of things I could in theory replace that I was paying for‚Ä¶ looking at you google drive/photos.\nSo as of writing this here‚Äôs everything that I‚Äôm running in my ‚Äúhome-lab‚Äù.\nFirst image shows the entire environment composed of my actual machines running and the second shows the stacks that in total run 38 containers.\n\nSome of the stacks in my environment.\n\nMost of my containers are managed via Portainer which is an excellent tool if you don‚Äôt want to keep tabs of the compose files for each service on multiple machines. All of the manifests are backed up to my personal Git server so in case of issues I can always get back up quickly.\nAnother thing to note is that I‚Äôm running everything in docker swarm, which I know is not ideal and mostly everyone would tell me to use kubernetes but still‚Ä¶ got the feel it‚Äôs an overkill for something so small.\nAs you will see in the service list below, I‚Äôve replaced couple services that I was paying for‚Ä¶looking at you Google Drive‚Ä¶and now I keep my own data in my own network, which is probably a bit risky given that my backups are still kept at home so in case of a disaster I will lose everything so eventually I‚Äôll need to figure out the 3-2-1 strategy, but that comes in the near future.\nBack to the services, here‚Äôs what I have replaced so far.\n\nNextcloud takes over Dropbox and Google Drive\nPhotoprism takes over Google Photos\nGitea replaces some functionality from GitHub, still use it but only for really external stuff.\nMinio replaces what little use I had in AWS S3. Works as a backup location.\nJellyfin replaces Plex, which I really never paid for but its OSS, has hardware transcoding out of the box, it‚Äôs really great.\nWallabag replaced pocket and any other bookmark URL service\n\nOn top of that I‚Äôm also running several services to keep Ads away, manage media and monitor my entire stack of computers. Some of the highlight services are.\n\nGrafana + Prometheus to monitor everything\nWikiJS to store notes of everything I‚Äôm learning\nPihole as explained in this other post\nUptime Kuma to be notified if a service is down or not working\nFileflow to convert media from various formats into a standard HEVC x265\n\nAnother important aspect that recently changed is the networking, as I used to have a port forwarded from my router to a machine listening (80/443) for HTTP/S traffic mostly, that lead to DNS leaks and privacy concerns so I‚Äôve recently switched to using Cloudflared tunnels, which allow me to still reach my services from outside my network and keep my IP private. On top of that I‚Äôve got Cloudflared managing some of the DNS requests with some rule sets so everything is secure and somewhat controlled.\nFor internal services not exposed that I need to access while away; I‚Äôve got a Wireguard VPN setup done so every time my phone is not on my WIFI network it auto connects to the VPN which gives me full access to everything in my home network on the go (this is done via Tasker), that gives me the use of features like that pihole to keep my internet experience somewhat safe. And since all of my traffic goes through the VPN I can feel safe when browsing or using public networks.\nI‚Äôm hoping to expand my home-lab a little bit more this year, would love to get a rack and some real enterprise servers but deep down that will be a bit too much and the wife will most likely complain about it, also I‚Äôm confident I will not use all of the resources those big machines offer; But for sure I‚Äôm in dire need of a NAS to keep all of my data backed up and available to all devices, right now I‚Äôve got stuff mounted via samba but its just not ideal so a real NAS hooked up directly in the network will help. Also a bigger server with more CPU and Memory as the Dell SFF and Pi are reaching capacity.\nMy Dell SFF sometimes peaks at 100% CPU when updating the packages so, I end up forcefully rebooting it and stop some services before updating the whole stack.\nMost of the media I‚Äôve got is transformed to HEVC 265 containers (checkout fileflow, it‚Äôs a pretty damn awesome tool) but some of the clients do not support that format so my little SFF has to execute media transcoding, which causes some CPU heavy workloads and as explained above can have some terrible outcomes, ideally the next machine I acquire would have a graphics card capable of encoding said formats or at least a CPU with the capacity to transcode the formats I need, so any Intel Gen 7 and above basically. I know my gaming rig with a GTX 3080 would do it, but that machine is only used for gaming, which as a parent of 2 kids doesn‚Äôt happen as often as I would like.\nFinally I‚Äôm sort of reaching a point were I think I‚Äôve got everything I always wanted running locally for now‚Ä¶ because this think is an addiction trust me, there‚Äôs still a big piece annoying me called authentication which I need to figure out (testing authentik right now to cover OAUTH and Proxy Auth) so maybe that will become a post in the future.\nHopefully this little write up either motivates you to take back control over your data or gives you the nudge you needed to grab that old laptop/computer and turn it into a small server. The learning curve is a fun ride and long term it may even help you land a job in IT!.\nIf you have any questions on why or how I‚Äôm doing some of these things please reach out.\nAdios üëã"},"posts/selfhosted-2024":{"title":"Self Hosted in 2024","links":[],"tags":["homelab"],"content":"It‚Äôs been 4 years since the self-hosted adventure began and it gets bigger and weirder as seasons come and go. What started with a single Dell refurbished machine (which i still use) to run certain services back in 2020 has now evolved into an actual mini rack with some serious network devices and more machines. So lets see what changed sine my last post back in 2022\nAn Actual Rack Arrived‚Ä¶\nBought a home in the crazy economy‚Ä¶ it was a wild ride but we are pretty happy with what we got, which meant that I no longer had to keep everything in the closet, this was a big change at a personal level but also on my self-hosting cause that meant i could actually get a rack to put everything that I owned which back then was still 1 Dell Machine SFF, 1 Dell 3080 and my Raspberry Pi. Also by having more space meant i needed to think seriously how I wanted my networking to look like.\nOriginally I thought a big Access Point connected to the router was going to be sufficient but having 3 levels and 20+ devices it wasn‚Äôt going to perform or scale well. So i went all in and acquired couple Unifi devices:\n\nDream Machine ‚áí Acts as a router and controller for other Unifi devices, love the fact it has way more features that lets me see whats going on in my network. Additionally it offers WIFI to the basement level.\nSwitch 60W POE ‚áí This is when i learned that some devices could be powered by an actual Ethernet cable and luckily for me the home already had Ethernet CAT5 cables all around so i just needed to patch the terminals and off it goes!. This is the critical piece that lets me connect all of my wired devices, like the actual machines that compose the services I run.\nAccess Point HD ‚áí Provides WIFI at an extended range, powered by POE and placed in the mid level, this single device connects 15+ devices on it own.\nIn-Wall Access Point HD ‚áí This was a weird addition since I wanted my office to be wired in so i needed to find a way to provide 2-3 Ethernet connectors for my work/personal computers as well as the Playstation for better performance. So it was placed at the top level in the home. This one also provides WIFI so i had all 3 levels covered.\nFlex Switch Mini ‚áí A single 4 port switch powered by POE that lets me connect the TV as well as other devices behind our main entertainment area.\n\nThis is how it looked it early on\n\nThe servers had babies\nWith the networking ready now I just needed to add more juice to the whole setup. A local Youtuber who makes a ton of awesome videos about self-hosting TechoTim was getting rid of an Intel NUC and I was lucky enough to be the first one to respond so I got a free powerful mini PC, many appreciations to him for being so kind.\nNext step was to solve a very simple problem, none of my machines were powerful enough to do video decode/encode (besides my main rig with a 3080) but that one isn‚Äôt on 24/7 so the next goal was to find a small machine that had enough processing power to allow me to encode media files on the fly. After much reading the solution was pretty simple, find a SFF (small form factor) computer that had an Intel processor 7th Gen or above. So  after checking my favorite subreddits (r/hardwareswap &amp; r/homelabsales) someone was selling an HP Gen4 machine with an intel 8th gen and 32GB RAM.\nThe Homelab was ready. We scaled from 2 to 4 machines + raspberry. So my computing power was ready for the workloads I had in mind.\nServices found a new home\nPreviously everything I ran was done via docker since it was way easier back then an also my Kubernetes knowledge was pretty limited, so with that in mind in late 2023 I made the jump and deleted everything running under docker (had all of the manifests backed up just in case) which at the time was around 20 stacks with 4X containers all the way from monitoring workloads like Prometheus  + Grafana + Elasticsearch all the way into Nextcloud and Photoprism to replace things I‚Äôve previously payed for. You can read more on what i ran back then here\nSince theres no better way to learn than doing I‚Äôve embarked on the Kubernetes path and started to build everything up again by generating the manifests that were basically composed of a deployment + service. Another big addition is that I fully embraced the GitOps flow so everything was being managed by FluxCD so it was a bit complicated at first to learn a brand new tool on top of a new container runtime and workflow, but it was worth the frustration as I now see the benefits of using this paradigm.\nMy deployments are now fully automated and in case I ever need to rebuild my entire Homelab all I would need is FluxCD to manage everything for me since all of the manifests now reside in a Github Repo .\nNew stuff I‚Äôm running\nWith everything automated and ready‚Ä¶ the self-hosting bug keeps increasing and ever time I see a new service pop up over in Reddit I have to try it. With the new workflow I can cook up a deployment + service and hook it up to my Cloudflared tunnel running within K8s and I can get a new service running in minutes.\nSome new service I‚Äôve found interesting:\n\nWindmill ‚áí Using it to build and automate some things I used to run in Cronjobs (scrapers mostly)\nUmami ‚áí Analytics for this blog you are reading on\nShlink ‚áí Generate links and track them\nExcalidraw ‚áí Well‚Ä¶ to draw things and I get to host it\n\nAlso worth mentioning some stuff I‚Äôm NO longer running:\n\nWikijs ‚áí Fully replaced it with Obsidian\nGitea ‚áí Migrated over to Github to make use of GHA - I do plan to self host my own runners.\nWallabag ‚áí Obsidian\nMinio ‚áí Cloudflare R2 - Minio was good but a pain to manage so its one of those i‚Äôd rather pay for.\nFileflows ‚áí I no longer have to worry about formats and encodings since i can do those on the fly without constant buffering\n\nFuture\nIn the near future I would like to improve the following:\n\nMy backing up strategies for the setup as well as the data, the NAS had a faulty disk sometime in 2023 and it took me awhile to recover from it.\nBalance the workloads, some nodes run more container due to the nature of some of the services so i need to make those more resilient.\nCreate VLANs so IoT and other type of devices do not access my entire network\nMake sure my external hosted services are secure with proper authentication so i no longer have to depend on keeping things internal‚Ä¶ sometimes i need access to something on the go\nRun my own LLM so I no longer have to use ChatGPT\nMaybe a 1U server so i know what it feels like to run an actual server and not just small computers\n\nThat‚Äôs pretty much it, a lot of things changed over the course of 2023 and with 2024 just starting the Self Hosted Journey continues strong.\nHope you enjoyed the update, see ya next time!\nAdios üëã"},"posts/server-scraper":{"title":"Ruby Server Scraper","links":[],"tags":["automation"],"content":"As shared in a previous post, been looking into expanding my existing homelab and the number of sites that sell used hardware is very limited and having to depend on sites like craiglist, Facebook marketplace was simply not going to cut it, cause first of all I no longer use my Facebook account and craiglist has always been fishy in my opinion‚Ä¶also have had some crappy experiences in there so no thank you. I had 2 options, either find something in reddit‚Äôs communities or keep an eye on a local store here in Minnesota that basically sells used hardware at very decent prices, shout-out to FGTW for having such sweet components and close to home - www.freegeektwincities.org/\nSo instead of me visiting the site every day, figured I could automate the entire process, cause that‚Äôs what software engineering is all about!.\nNow I‚Äôve done web scrapers before in python using beautifulsoup and requests, I wanted to try to do it in another language so Ruby was chosen given on how much I have to use it at work figured it would be a good problem to solve, because that‚Äôs how you really learn a new language, by solving a problem you currently have.\nEnter Ruby\nRuby feels familiar to me in a sense that I can almost read it as as if it were a book, similar experience with Python, with some minor syntax changes which I won‚Äôt explain here but a quick search will you tell you the main things that sets them apart.\nI would argue it‚Äôs a good first language for anyone to learn. Its OOP based and has a very active community with tons of packages that do all sorts of magical things. Has some good and cherished frameworks so give it a try.\nSo that being said, here‚Äôs the logic for the scraper.\n\nStart a web process/make a request to the site with HTTParty - equivalent to requests.\nParse the contents of the site using nokogiri - equivalent to beautifulsoup.\nStore the results in a sqlite database on the first run.\nEvaluate the results on each execution.\nSchedule it on a server to run every X hours.\nIf a new server is found, notify me via telegram.\n\nNOTE: When Scraping sites make sure you don‚Äôt spam them with requests every X seconds, some sites have policies against this so do it with caution and some common sense people.\nWill not go through every single line of code so here‚Äôs the gist of it!.\nWe first got to init a DB and create some tables, this is one of the very few scripts I have developed that actually keeps some sort of state, most of the other quick ones just dump stuff either on console or a txt file somewhere, but given that I do not want to see the same results over and over, some sort of state was needed.\n# Create a DB in the user directory and start the table\ndef init_db\n  db = SQLite3::Database.open &quot;/home/#{ENV[&#039;USER&#039;]}/git/server_scraper/tracker.db&quot;\n  db.execute &#039;CREATE TABLE IF NOT EXISTS products(product TEXT, added TEXT)&#039;\n  db.results_as_hash = true\n  db\nend\nWith the records in the database now we needed to know when a new server was posted and since I already use telegram to get notified for other situations like my server up-time or when new media is available and when other scripts that automate tasks complete (backups, cleanup, updates, etc.), we could basically use the same logic to get notified when new stuff drops.\n# Start a telegram client\ndef init_telegram(token, log)\n  if token.nil\n    log.fatal &#039;Token not loaded, aborting&#039;\n    abort\n  end\n  client = Telebot::Client.new(token)\n  # Check if client started correctly\n  log.error &#039;Bot not initialized&#039; if client.get_me.nil\n  client\nend\nThe meat and potatoes of the script that ties it all together. In this case no classes were needed cause this is supposed to be simple yet effective.\n# Launch a request, store the time and parse the results\n \ndef main(db, client, chat_id, log)\n  # Scrape results from site\n  log.info &quot;Starting script at: #{Time.now}&quot;\n  url = &#039;www.freegeektwincities.org/computers&#039;\n  request = HTTParty.get(url)\n  html = Nokogiri::HTML(request)\n  products = html.css(&#039;.ProductList-title&#039;)\n  log.info &quot;Found #{products.length} products in site&quot;\n \n  # Validate and Save\n  products.children.each do |p|\n    check_if_db = db.query &#039;SELECT * FROM products WHERE product = ?&#039;, p.inner_text\n    log.info &quot;Checking if #{p.inner_text} exists in the database&quot;\n    if check_if_db.count.zero\n      # Save to DB, notify and log\n      log.info &quot;Server added to DB: #{p.inner_text}&quot;\n      client.send_message(chat_id: chat_id, text: &quot;NEW SERVER POSTED:#{p.inner_text}&quot;)\n      db.execute &#039;INSERT INTO products(product, added) VALUES (?,?)&#039;, p.inner_text, Time.now.to_s\n    else\n      log.info &#039;Server already in the db, skipping&#039;\n    end\n  end\n  client.send_message(chat_id: chat_id, text: &quot;NO NEW SERVER FOUND at #{Time.now}&quot;)\nend\nThe secrets and tokens are kept as environment variables and loaded when the script is executed. The rest of the code can be found in the repo here.\nFinal Thoughts\nThe script is configured via systemd-timers to run every 4 hours which I believe is pretty fair to the site and to my use case. Did struggle a little bit with what CSS selectors to use to actually grab the data from so that it could be stored in the DB, but that‚Äôs the joy of scraping honestly.\nWould say that this script took me maybe like 2 hours from creation until deployment and overall taught me a lot about the language, like how importing works and how to best use the famous ruby blocks. Also how the entire bundler ecosystem works, what it means to me as a user and how to use it.\nNow thanks to this little script I ended up finding a new Dell 7020 computer with a beefy CPU and 32GB of RAM, so now I can use hardware transcoding on my media, spread some of the heavy services that the Dell SFF and Raspberry PI couldn‚Äôt just keep up with (Prometheus kept restarting once a day, annoying‚Ä¶).\nSo hopefully you found this somewhat interesting. Remember that the best way to really learn a new technology or language is to actively use it to solve a problem you have!.\nAdios üëã"},"posts/speedtest-kibana":{"title":"Monitor your ISP Speed in Kibana","links":[],"tags":["elk","homelab"],"content":"With the working from home situation my wife has been complaining that the internet is somewhat slower which makes sense since we now have more devices on and streaming content‚Ä¶ I have watched Frozen 1 and 2 over 30 times now‚Ä¶ but figured I should double check to see the actual numbers, so I consulted the good ol‚Äô site speedtest.net and ran couple tests and noticed that I am indeed getting sometimes half of what I‚Äôm paying for and some other times I do get more than I should. As FYI I‚Äôm paying $45 for 100 mbps which is pretty nice in my opinion.\nEither way, I had no intention to keep visiting the site every so often and when my wife said it was slow I didn‚Äôt want to check if my ISP was working properly so I discovered there‚Äôs a CLI that executes that test we all know and love in the site and displays the results in machine readable formats. So here‚Äôs when the engineer (lazy) part of my brain kicked in and gave me an obvious solution‚Ä¶ automate the whole thing. So overall here‚Äôs what I said I would do.\n\nFirst step, get the CLI and run it couple times to know what the parameters you are looking for do, in my case i wanted something that would execute the test against a not so local server since it‚Äôs not realistic that most of my internet requests end up couple miles from where i live.\nSecond step, dump the data into a file that can be easily parsed and ingested into a system, in this case my preference is Elasticsearch since I can keep the data and I‚Äôm quite familiar with it so building the mapping and visualizations is super simple. The ingestion can be done by either writing a script to push it out or you can do it the lazy way and setup a filebeat agent that will simply collect the data from the results file in json format and will push that directly into my existing Elasticsearch cluster.\nThird step, generate some graphs to visualize how my ISP is treating me.\n\nI won‚Äôt go much into detail as this is supposed to be a quick post, but do let me know if you get stuck somewhere, social media details at the bottom.\n\nDownload the CLI from the official site - www.speedtest.net/apps/cli\nEither create a cron task to run the script every X minutes or use a systemd timer unit (this depends on your preference completely). Here‚Äôs my systemd timer and service files in case you want to copy it. Got place it in /etc/systemd/system/ and start &amp; enable it using sudo systemctl enable speedtest.timer this should take care of calling the service every 10 minutes which is plenty for my case.\n\n[Unit]\nDescription=Run the speed test cli\n \n[Service]\nUser=$USER\nGroup=$USER\nExecStart=/home/$USER/speedtest/speedtest --server-id=2917 --format=json -u MiB/s\nStandardOutput=append:/home/$USER/speedtest/speedtest.log\nType=oneshot\n \n[Install]\nWantedBy=default.target\n \n \n[Unit]\nDescription=Runs speedtest every 10 minutes\n \n[Timer]\nOnCalendar=\\*:0/10\nUnit=speedtest.service\n \n[Install]\nWantedBy=default.target\n\nWith that in place the script should place the output of the file under your home folder so make sure it exists.\nNext, you need to have filebeat running and configured to keep tabs in that speedtest.log file. Here‚Äôs again a snippet of my configuration for both filebeat.yml and the service file that runs it.\n\n[Unit]\nDescription=Filebeat\nDocumentation=www.elastic.co/guide\nAfter=network.target\n \n[Service]\nType=simple\nRestart=always\nUser=$USER\nGroup=$USER\nExecStart=/home/$USER/apps/filebeat-7.5.2-linux-x86_64/filebeat -c /home/$USER/apps/filebeat-7.5.2-linux-x86_64/filebeat.yml\n \n[Install]\nWantedBy=multi-user.target\nSince the file is already formatted as JSON we will tell filebeat we want to decode it as is and place the keys under root so we can search them, also I‚Äôm adding a field for those events so I can filter as needed. It‚Äôs always a good idea and practice to ‚Äútag‚Äù your data.\nfilebeat.inputs:\n  - type: log\nenabled: true\npaths:\n  - /home/$USER/speedtest/speedtest.log\nfields:\n  source_system: speedtest\nfields_under_root: true\njson.keys_under_root: true\n\nWith the data inside the cluster, all you got to do next is just build up some visualizations. Here‚Äôs some of the ones I built using those details. Now I got a holistic view of how my ISP is treating me, can get historic data on how it behaved, the number of packets and ping I‚Äôm getting, it‚Äôs quite nice.\n\n\nAs you can tell data is beautiful and with Kibana it‚Äôs quite easy to visualize, this was a quick on how to ingest something from a script and use it to your advantage. I would like to also collect all of the packets coming in and out for all of my devices so I could measure my usage as well but that requires custom devices used as gateways and more complex setups.\nSee you in the next one."},"posts/split-keyboards":{"title":"Split Keyboards are fun","links":[],"tags":["random"],"content":"It‚Äôs been 3 months or so now from when my journey into the split keyboards started and I blame my old cheap self and tech twitter who once again managed to ‚Äúinfluence‚Äù me into buying things I probably should‚Äôve acquired a long time ago.\nI‚Äôm not exactly a coder that is writing non-stop for hours like some of you might be, but I do type a lot to communicate over Slack and also I‚Äôm probably the top person on my team doing documentation and updates on playbooks or notes, so over time I started to develop stiffness on my wrists after a good session at work. Always imagined it would take me couple more years to reach the stage of having physical pain at work (emotional pain is always there).So after watching streams, YouTube videos and countless posts on tech twitter about how split keyboards were the #1 thing people recommend you get if you want to make it into the world while also preventing a lot of pain down the road, it was my time to join the club.\nSo as a birthday excuse my wife insisted on buying something ‚Äúnice‚Äù, so I pulled the trigger and ordered a ZSA Moonlander MK I. Which covers some of my requirements:\n\nIt‚Äôs split duh!\nCustomizable keycaps and switches\nCan be programmed to do a bunch of things\nHas RGB - yes I‚Äôm pathetic like that!\nExtras can be either printed or bought directly from the vendor\nLooks awesome, it just does.\n\n\nNot going to lie the first days I couldn‚Äôt even type more than 4-7 words per minute because everything was confusing. I had to keep looking at the keys, so I could narrow down which keys to hit. Also, idiot me decided to buy the version with blank keycaps so my pain and frustration levels went through the roof, thankfully I had some spare keycaps from my Razer Gaming Keyboards and those worked while I settled into things.\nAnother big time consumer item I wasn‚Äôt truly expecting was finding or setting up the keys into locations I could easily find, remember and benefit from. Spent couple hours just flashing over and over the board cause realized I either couldn‚Äôt find a key or it was on a location that made no sense to me. Eventually I found my silver bullet configuration which you can see here.\n\nOverall it took me 1.5 months to get back into my average words per minute (90-105) with minimal errors.\nAlso, those long documentation or blog writing sessions no longer cause problems.\n\nFinally, my setup just looks awesome with the split keyboard. I‚Äôm really happy with my purchase.\nTLDR - Yes the keyboards are expensive but, they are awesome, fun and help you a ton in the long run.\nUntil the next one!\nAdios üëã"},"posts/splunk-docker":{"title":"Splunk Quickstart Guide","links":[],"tags":["splunk","container"],"content":"An opportunity came up at work for me to expand my tool set into another logging solution that is quite popular, Splunk.\nKnown to be a bit expensive cause of the license fees and the model they implement for enterprise solutions I was pretty amazed on what it can do and given the experience, I have with the competition Elasticsearch\nit was a complete 360 on how I knew data was pushed into the system and leveraged some components and functionality have their similar set of functions comparing it to Elastic‚Ä¶but the main difference to me is how Splunk\nmanages a schema on read which sort of translates to‚Ä¶there are not a lot of fields and you have to create them on your own when searching.\nFor an actual detailed explanation you can take a peek at the docs, in here we do hands on type of posts.\nThis demo is made using the free license that allows us to push up to 500 MB to the Splunk instance before incurring into license problems which for most cases should suffice.\nThe Setup\nSince I‚Äôm not a fan of downloading tarballs and setting up a lot of things (users, permissions, service files) I‚Äôm going to leverage containers that are packaged with all of the goodies so here‚Äôs what I‚Äôll be using.\nversion: &#039;3&#039;\nservices:\n  splunk:\n    image: splunk/splunk\n    environment:\n      - SPLUNK_START_ARGS=--accept-license\n      - SPLUNK_PASSWORD=mysuperstrongadminpassword\n    ports:\n      - &#039;9997:9997&#039;\n      - &#039;8000:8000&#039;\n    volumes:\n      - /opt/splunk/var:/opt/splunk/var\n      - /opt/splunk/etc:/opt/splunk/etc\nThe only thing I‚Äôve setup was the /opt/splunk/var and /opt/splunk/etc/ mount points in my local server so I could have those configurations easily accessible for me to adjust and tweak.\nWe also need a port so we can push the data into Splunk via agents called forwarders that need a port to connect to so we are mapping 9997 which is the default.\nThe last piece is that we need a port so that we can connect to our instance via the web UI so we have 8000 mapped as well.\nOnce you download the big image and it does all of the checks it needs to, hop over to http://localhost:8000 and you should be greeted by the login page in here you will use username admin and the superstrongpassword you setup in your environment variables.\nPushing data in\nThere are multiple ways to push data into Splunk, you have‚Ä¶\n\nAgents (heavy and universal forwarders)\nThe HEC( HTTP Event Collector)\nTCP &amp; UDP\nScripts\n\nFor this post we are going to leverage the Universal Forwarder agents.\nBy default the Splunk installation can read files from its local setup meaning it has a forwarder built in but since we have it trapped in a container we can‚Äôt get much out of it.\nThe goal in this post is to have the instance read logs from 2 different machines, my local server where I run all of my containers and my gaming rig running windows, for both of these I will use the universal forwarder that you can download\nfrom Forwarders keep in mind you need to create an account with Splunk.\nMake sure you pick the right agent for the OS you will be working with again in my case I‚Äôve downloaded both tarballs and the Windows MSI.\nBefore actually pushing data in we have to setup the receiving functionality in Splunk so head over to the Nav Top Menu and go to Settings. In the drop down you should see a ‚ÄúForwarding and receiving‚Äù link,click on that and then go to ‚ÄúConfigure receiving‚Äù.\n\nInside that menu you will click on ‚ÄúNew Receiving Port‚Äù and all that you will need is to define the same port we mapped over in our container 9997.\n\nNow before we get the agents setup we have to do one final but critical step, create the indices that will receive the data, this is something i wasn‚Äôt fully aware of that in SPLUNK you need to create the indices FIRST and then setup your agents, the indexes are NOT AUTO-GENERATED unlike Elastic. Had setup my agents and I couldn‚Äôt see any data‚Ä¶ couldn‚Äôt figure it out, suffice to say‚Ä¶ wasted couple hours learning this the hard way.\nGo to Settings ‚Üí Indexes and create those Index, in my case I‚Äôm going to do one for all of my windows events and one for linux.\n\nNow the agents\nThe installation is windows is pretty straightforward, you click on Next until its done using the default settings. For the configuration of the inputs and outputs we will make use of the CLI.\nFor installing the agent on linux you simply unpack the tarball and place it somewhere you like, I personally always dump everything out to /opt. Same as with above we will configure this agent via the CLI.\nThese commands work the same regardless of the OS the agent runs on. We will assume your agents are installed in locations:\n- C:\\Program Files\\SplunkUniversalForwarder\n- /opt/splunkforwarder\nIn both cases you can get to the binary by going to the ‚Ä¶ bin folder\n# Start the service, you will be asked to setup a user and password for the local agent, remember those credentials\n./splunk start --accept-license\n \n# Add the forwarding server that will receive your events, you will need to know the &lt;IP-of-your-host-running-splunk&gt;\n./splunk add forward-server &lt;IP&gt;:9997\n \n# Confirm the forward server, you should see something like\n./splunk list forward-server\nActive forwards:\n        192.168.0.22:9997\nConfigured but inactive forwards:\n        None\n# To tell it to &quot;monitor&quot; some files, you just pass in your path or filename\n./splunk add monitor &quot;/var/log/*&quot; -index linux\n \n# To verify the monitored files and folders\n# Splunk monitors itself so you will see a big list of files in here but yours should be there too\n./splunk list monitor\n   /var/log/*\n                /var/log/audit\n                /var/log/btmp\n                /var/log/btmp.1\n                /var/log/fluentd\n                /var/log/gssproxy\n                /var/log/journal\n                /var/log/lastlog\n                /var/log/lighttpd\n                /var/log/nextcloud\n                /var/log/nextcloud/audit.log\n                /var/log/old\n                /var/log/pacman.log\n                /var/log/private\n                /var/log/squid\n                /var/log/wtmp\nBy default these commands generated a set of files that are CRITICAL to how the agents work‚Ä¶ the inputs.conf and the outputs.conf which are the list of what it will monitor and where it will send it to. Since we have custom indices we can validate that the files contain the same stanzas that we declared in the CLI.\nThe files are usually located inside the respective etc/system/local/ folders\n# inputs.conf\n[monitor:///var/log/*]\nindex=linux\n \n# outputs.conf\n[tcpout:default-autolb-group]\ndisabled = false\nserver = localhost:9997\n \n[tcpout-server://localhost:9997]\nFor windows, most of the relevant things I wanted to monitor reside inside the windows event logs so I manually created the inputs.conf with the following stanza. Pretty simple it will read from these 3 facilities and ignore everything older than 3 days and push those into my custom index.\n# windows inputs.conf\n[WinEventLog://Application]\nindex=windows\nignoreOlderThan=3d\n \n[WinEventLog://Security]\nindex=windows\nignoreOlderThan=3d\n \n[WinEventLog://System]\nindex=windows\nignoreOlderThan=3d\nWith everything setup we may now restart the agents ./splunk restart and we should see data in Splunk.\n\nConclusion\nI hope this post guided you on how the ingestion setup works in Splunk, the multiple components that are involved in the flow and the overall functionality of it all. My next project is going to be pushing my docker logs into Splunk and of course learning the extensive language that Splunk uses to extract, graph and visualize the data. Because remember that unless you create a very specific parsing pattern using sourcetypes you will not see default fields.\nIf you have any questions or comments, hit me up on twitter, linkedin, etc."},"posts/steam-deck-awesome":{"title":"The Steam Deck is awesome","links":[],"tags":["random","gaming"],"content":"Thanks to my dumb luck and a recent trip up to Seattle (The best state to live in according to people I know), I ended up with $300 worth of Steam Credit and amazingly enough is when the Steam Deck OLED decided to drop so by me injecting couple hundred dollars more I ended up with a shiny new Steam Deck OLED 256 GB.\nOriginally the plan was to receive the device, post it on a marketplace and recover my investment of those $300 I couldn‚Äôt take out of the Steam Market‚Ä¶but dumb me got the device and decided to open it to make sure the screen and everything was intact cause you never know with packages these days, was a bit more worried cause the outside box looked a bit banged up.\nWell the device was perfectly fine, protected by its case, and it had that new plastic smell so that tempted me to see how good the device was. The last handheld device I played was a Nintendo Game Boy‚Ä¶ yeah I‚Äôm that old.  Steam loaded up quickly and the Deck felt so smooth, the screen was super bright so figured I‚Äôd give it a test with a quick and lightweight game that I‚Äôve had in my catalog for years now; Hollow Knight.\nThe first 30 minutes were enough for me to get addicted to Hollow Knight, how could I not play this game when it came out‚Ä¶ probably bought it 3 years ago, and it looked cool but never game myself the chance to play anything else but stupid DOTA. How wrong was I‚Ä¶ it‚Äôs such a magnificent game, at this point I already completed the game at 102%, and I still have 10% more to go to really really finish it.\n{%meme src=‚Äúvgkami.com/wp-content/uploads/2022/04/hollow-knight-memes.jpg‚Äù /%}\nWith that in mind, figured maybe lets try more games?. Stupid me ended up downloading 5 more and they all work beautifully, never before have I felt such joy playing games and the fact that I can do it on the go or anywhere really is so refreshing.\nWhile talking with my brother who loves consoles, he pointed out that you can also play non-Steam games which blew my mind cause deep down this is just a Linux machine running SteamOS right‚Ä¶ so one hour later I had Diablo 4 and Diablo 2 Remastered up and running.\nThe deck is seriously an amazing device, I‚Äôve become that guy that can play in the sofa or the bed‚Ä¶ sometimes maybe even in the bathroom.  Suffice to say the Deck is now mine, and it‚Äôs not going anywhere. Turned out to be an excellent Xmas gift to myself.\nSo TLDR - The Steam Deck is awesome and revitalized the gamer in me. Also turned me into a monster that is addicted to metroidvania games.\nHades is such a good game too!. üòâ\nSee ya!"},"posts/streaming":{"title":"Started Streaming","links":[],"tags":["random"],"content":"It has been years since I discovered Twitch, and I regularly use it to watch tournaments of my favorite games or observe others trying out new games. This allows me to validate if it‚Äôs worth my time and money because watching reviews is cool and all, but seeing someone else play it just hits differently.\nA couple of years ago, I found out about the ‚ÄòSoftware and Game Development‚Äô category, which seemed like a gathering of my type of nerds doing their thing in public. I was hooked and started to regularly tune into streamers like Prime, Teej, Bashbunni, and TheAltF4Stream. It was a great way to kill time, get motivated, and see how other people work in public. Additionally, I learned new things because they would either develop something or try out new languages and frameworks. It looked like a ton of fun.\nFast-forward to 2023 when I actually met some of them. While talking, they were super nice and mentioned that I should give it a go as well (Thanks BG!). Initially, it was something I kind of wanted to do, but I didn‚Äôt feel confident in doing. Until the end of 2023 when I just said YOLO and ordered a microphone, a mic arm, and a cheap light to hang over my monitors so people could see me.\nOne night, I clicked on ‚Äústart streaming‚Äù in OBS, and so far, I‚Äôve done it on a regular basis after my kids go to sleep from Monday to Wednesday. I already have 20 hours of stream content, which seems low, but it‚Äôs a big achievement for me.\nThe first couple of times, audio problems haunted me‚Äîeither the music was too loud, the microphone was way too low, or I flat out forgot to turn it on. That was a fun stream. I started to get myself involved in more OBS settings and overlays. A couple of days ago, I also bought a stream deck which I‚Äôm using to switch scenes in OBS, adding tons of fun.\nOne thing to point out: I‚Äôm not expecting to get famous or rich from this. I‚Äôm honestly doing it because I find it fun, and I really don‚Äôt care if there‚Äôs no one seeing me. I will admit, though, I still get nervous when someone randomly pops up and asks a thing. I have to remind myself to try to interact with people because, at the end of the day, I need to build up the confidence.\nSo, if you are interested in seeing me and my late study sessions, tune in and ask stuff or just troll me.\nTwitch\nSee ya!"},"posts/switched-back-to-windows":{"title":"Windows is decent again?","links":[],"tags":["random"],"content":"\nBeen using windows again for almost 2 months now and I gotta hand it to them, it has improved quite a lot. Last time I had it installed on my system was around 2017 when at some point I wanted and needed to play more with the LINUX operating system for work purposes so I could understand it more and honestly I was a bit tired of the constant updates that took forever‚Ä¶ additionally had always been tempted to make the switch so I backed up everything and kissed goodbye to my old OS.\nI did miss the one windows perk which was playing AAA games, at the time I was super into PUBG and it was and I believe still is unplayable in Linux but soon after something called Proton‚Ä¶started to pop up more and more, turns out at the beginning of 2020 I could play almost *everything* in my Steam library using my Linux OS except couple new and fancy games that I was dying to play and couldn‚Äôt be emulated properly, so I entirely blame the switch on Doom Eternal and COD Warzone.\nSo I downloaded a copy of windows and created the image on a boot drive and so it began‚Ä¶ wiping the drives and installing Windows once again, it took almost 30 minutes including the number of restarts it has to do to make sure everything is done properly and setting up the accounts and whatever else it does, finally it was complete and I was presented with a fresh copy of Windows, it still had a bunch of bloatware I did not need and I still feel it shouldn‚Äôt even be installed on the system to begin with but that‚Äôs for another day. Now I do have to add, I kept using the Windows at work and with new additions like WSL the switch was even more tempting cause I could use it without being blocked by company policies in the work equipment.\nOne of the great perks of Linux distributions is the package managers (yum, apt, pacman, etc) which is something that I always wished windows would have so I could automate everything like I did before. So googling around I found that windows is already working on it‚Äôs own package manager which was awesome but for the meantime I also found chocolatey so I quickly looked up my most common apps and installed them incredibly easy, no need to download a bunch of installers or to get them from dubious sites, again in a matter of minutes my system was ready.\nAn extra item I did not expect was that the Windows Terminal was pretty decent, I mean it‚Äôs not going to compete against Konsole in KDE but for being their first try it‚Äôs damn good, you can blur the background, change the colorscheme, use custom fonts and it has tabs that take you directly into a WSL distro, once again, very impressed. I have uploaded my personal configuration file to my dotfiles in case you are curious.\nThe cherry on top was installing WSL, by the time I switched WSL2 had just come out so I enabled it and installed it, got docker running on top of it and got 2 distros to play with (Ubuntu 20.04 and Kali), hell I even ran my ansible playbooks against one of the distros just to install some software as well and everything was working as expected, I was honestly shocked at how good the performance was, I mean I‚Äôm not doing hardcore stuff but to have my containers running over and all of my software dev kit installed and running from WSL2 into windows was just magnificent. This post was written in Vscode running in Windows but connected to a WSL2 backend, how crazy is that!?\nAt the end I had it all:\n\nI could play my AAA games like Doom and CODWZ\nI could quickly jump into my WSL2 Distros to play and try new technology.\nI can install a lot of things from the CLI using chocolatey and I‚Äôll be waiting for the Windows official version.\nI have a pretty good looking terminal.\n\nI‚Äôm honestly very pleased with it now and with the amount of things coming down the pipeline for windows I am for once excited about the future of it, like running GUI apps from within WSL, the package manager and if they clean up their privacy stuff I believe it could start pulling some of the market of users like me that want to develop things but also enjoy playing some good games."},"posts/tailscale-is-cool":{"title":"Tailscale is pretty cool","links":[],"tags":["homelab"],"content":"Big part of owning a Homelab is running a ton of services for myself or the family, some of which can be and should be accessed via the internet but some of them are private so, I‚Äôd like to still figure out a way to get to them without exposing or doing weird reverse proxy magic to filter requests/headers.\nI eventually discovered Wireguard, and it was the perfect solution, took me couple minutes to set up on a machine, laptop and my phone, with one caveat a port had to be exposed within my router (port 51820) and suddenly everything local was available on my phone or on my personal laptop while sitting at a coffee shop drinking Chai tea like a total chad; Life was good.\nUntil someone on a podcast which I can‚Äôt quite remember anymore, mentioned Tailscale‚Ä¶ shrugged it off as my solution was working so why change. Then I saw/heard about it over and over on the r/homelab sub-reddit, so curiosity was peaked and as someone that like to tinker a lot I decided to just give it a try, worst case scenario I have my good ol‚Äô reliable Wireguard setup in a compose file to re-apply in seconds right?.\nSuffice to say I was impressed with it. It‚Äôs like Wireguard but without the exposed port and a better UI/App.\n\nTwenty minutes later I had the exact same setup on my 3 devices, everything was working as intended even my local DNS which was a bit of a hassle to figure out on Wireguard natively since those local records reside on my Pi-hole, which btw you need to set up ASAP!.\nNow since Tailscale is a paid product of course it has some extra features that allow you to expose routes, a service/CLI that gives you specifics on traffic and clients, split tunneling and user management, etc. Don‚Äôt want this to become an AD for them (not paid or sponsored in any way).\nSo to conclude, it is a great product, and it uses the same technology deep down (wireguard).\nIt keeps getting updates and features that most of us can leverage without paying a dime for your homelab or small network. The Android App just got a new UI which is beautiful and easy to automate with something like Tasker, having the VPN turn on as soon as I leave my Wi-Fi is just magic.\nWould highly recommend giving it a shot, I believe there‚Äôs a fork on it that is FOSS, so maybe that‚Äôs more your cup of tea - github.com/juanfont/headscale.\n\nThe networking and how to access services in your Homelab is one of the most complicated thing unless you have deep expertise on the subject, so I‚Äôm grateful that we have products that facilitate the access.\n\nAnyway, back to some more homelab-ing. Hope this helps you reach into your network easier!.\nAdios üëã"},"posts/traefik-guide":{"title":"Traefik Quick Start","links":[],"tags":["homelab"],"content":"In my previous post talked about how i ended up using Traefik instead of the good ol‚Äô reliable NGINX, so wanted to expand a bit more for people that may want to test this out and not want to spend hours like i did testing and reading documentation, not saying you shouldn‚Äôt cause you definitely will but if you want something quick, then this guide is for you.\nCreate your network\nAll containers that Traefik will expose need to be on the same network so if you are using something like swarm or compose, make sure you have an ‚Äúexternal‚Äù network that all containers can reach.\nNOTE: By default, all compose files that do not define a network will end up generating a network that will have the name of the first service in your file, this is useful so that everything in that compose file can talk to each other but in our case, it just pollutes our docker engine with more stuff to manage.\nCreate your overlay or bridged network\n# SWARM\ndocker network create traefik-proxy --driver overlay\n \n# No SWARM\ndocker network create traefik-proxy\nSetup Traefik\nTraefik relies heavily configurations on either a static file that you can mount to it or by using labels, which I honestly prefer (this is referred to as dynamic configuration by them).\nversion: &#039;3.7&#039;\nservices:\n  traefik:\n    image: traefik:v2.0\n    networks:\n      - traefik-proxy\n    command:\n      - --entrypoints.metrics.address=:8082\n      - --entrypoints.http.address=:80\n      - --api.dashboard=true\n      - --api.insecure=true\n      - --providers.docker\n      - --providers.docker.swarmMode=true\n      - --providers.docker.exposedByDefault=false\n    ports:\n      - 80:80\n      - 8080:8080\n      - 8082:8082\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    deploy:\n      placement:\n        constraints:\n          - node.role == manager\n \nnetworks:\n  traefik-proxy:\n    external: true\nA quick breakdown of what these mean. A more detailed explanation can be found on the documentation - here.\n\nIndicates that the service will use the network called ‚Äútraefik-proxy‚Äù.\n\nnetworks: - traefik-proxy\n\nWe define an ‚Äúentrypoint‚Äù aka a port that we can hit to reach a service, and we name it ‚Äúhttp‚Äù since its using port 80.\n\n- --entrypoints.http.address=:80\n\nTraefik comes with a nice dashboard that helps you visualize what is running and the health of the services behind it, I recommend using it. So this line just enables it and allows you to reach it without using authentication\n\n- --api.dashboard=true --api.insecure=true\n\nSince we are running this in a docker container we need to tell traefik to listen to events in the engine, so we enable it.\n\n- --providers.docker\n\nIn my case I run everything in SWARM mode so I need to enable the mode, if you are not, you can skip it.\n\n- --providers.docker.swarmMode=true\n\nFinally, by default Traefik will try to match the containers against certain rules so it can expose those services, and since I didn‚Äôt want to expose everything I had to turn this off. If you don‚Äôt do this you will see warning messages in your container logs about not having default rules set for every service.\n\n- --providers.docker.exposedByDefault=false\n\nTo allow Traefik to listen in to the events for all of your containers we need to share the docker socket.\n\nvolumes: - /var/run/docker.sock:/var/run/docker.sock\n\nFinally since we have an external network created we need to indicate that we want to use it.\n\nnetworks:\n  traefik-proxy:\n    external: true\nWith that, we have our configuration set, now to configure a service\nSetup your services\nIn the example, we will be setting up a Grafana instance.\nversion: &#039;3.3&#039;\nservices:\n  grafana:\n    image: grafana/grafana:latest\n    environment:\n      - &#039;GF_SECURITY_ADMIN_PASSWORD=SuperSecretPasswordMan&#039;\n    networks:\n      - traefik-proxy\n    volumes:\n      - /opt/grafana:/var/lib/grafana\n    deploy:\n      placement:\n        constraints:\n          - node.labels.name == pi\n      labels:\n        - &#039;traefik.enable=true&#039;\n        - &#039;traefik.http.routers.grafana.entrypoints=http&#039;\n        - &#039;traefik.http.routers.grafana.rule=Host(`grafana.local.net`)&#039;\n        - &#039;traefik.http.services.grafana.loadbalancer.server.port=3000&#039;\n        - &#039;traefik.docker.network=traefik-proxy&#039;\n \nnetworks:\n  traefik-proxy:\n    external: true\nAnother breakdown. Same as before all of the configuration can be done via labels, which makes traefik so cool to use.\n\nFirst thing, we run the service in the network where traefik can reach it.\n\nnetworks: - traefik-proxy\n\nWe allow traefik to route the traffic for this service.\n\n- &quot;traefik.enable=true&quot;\n\nIndicate which entrypoint your service will use, do note that each route you define must be unique, in the example I called the route ‚Äúgrafana‚Äù so replace it in your configuration as needed.\n\n&quot;traefik.http.routers.grafana.entrypoints=http&quot;\n\nThe name of the route that will be used to redirect your request to the service, in my case i have a ‚Äúlocal.net‚Äù domain running so i ended up just giving each service a naming convention of ‚Äúservice_name.local.net‚Äù\n\n&quot;traefik.http.routers.grafana.rule=Host(grafana.local.net)&quot;\n\nMost services expose a port so we have to tell traefik which port will be used to redirect the traffic to, Grafana does it at port 3000.\n\n&quot;traefik.http.services.grafana.loadbalancer.server.port=3000&quot;\n\nA bit redundant but we indicate the network we are using this one as a label.\n\n&quot;traefik.docker.network=traefik-proxy&quot;\n\nFinally, force the entire service to use an already defined external network.\n\nnetworks:\n  traefik-proxy:\n    external: true\nValidation\nWith traefik and our grafana service deployed and running, we can now validate with the built-in dashboard to see if our instance was picked up properly as below.\n{% image src=‚Äútraefik-dashboard.png‚Äù alt=‚ÄúTraefik dashboard‚Äù /%}\nOur service in detail, the route and the service.\n\nIn our browser, if we navigate to grafana.local.net we should in theory now be presented with the default grafana setup. In case you don‚Äôt see it you might have DNS problems, the easy workaround is to add the name of the site to your http://localhost so it resolves the traffic.\n# /etc/hosts\n127.0.0.1 localhost grafana.local.net\nIf you run your own DNS server like me you can simply add a CNAME record that points to the server that hosts the service, done properly you should see.\n\nConclusion\nHope the mini-guide helped you out on how to set up your services, once you have one running it should be easy to replicate by simply copying the configuration and replacing couple values. The traefik documentation also provides a couple of examples in case you wanted to have your routes setup like domain-name.com/grafana. Finally, you could also have some middleware for your services in case you want to put some authentication before a user can get to your service which is always a good idea if you end up running things for a bigger group or PROD.\nThen again I‚Äôm the only one that uses these at home so why bother right?\nAs always if you have questions you can reach out to me on social media."},"projects/asdf-tea":{"title":"Introducing asdf-tea","links":[],"tags":[],"content":"I‚Äôve been using asdf a lot lately to manage all of my languages versions and random packages that simply do not have an easy way to install\nor are just convenient to have. So with that in mind I‚Äôve tried to adapt the CLI workflow for my git projects, and I‚Äôm already pretty used to using gh for GitHub,\nbut since most of my projects live on my personal Gitea instance I was looking for a similar experience where one could simply create PRs, Issues, etc.\nThankfully the Gitea project offers a similar CLI that can almost go toe to toe vs Github but there was a minor problems. You needed to download the compiled binary and then put it in your path‚Ä¶ give it the proper permissions and that was just too much,\nSo I spent an hour or so reading how asdf plugins work and decided to make my own.\nIf you are curious here‚Äôs the list of what i currently install with asdf:\n\ngolang\nhelm\njq\nkubectl\nlua\nnodejs\nruby\nrust\nstern\nterraform\nterragrunt\nawscli\nsops\nlazygit\ntflint\n\nLike I said mostly languages and utilities.\nSo now.. Introducing asdf-tea‚Ä¶which is pretty boring in essence cause ultimately it just downloads the binary and lets asdf handle how it‚Äôs called and installed‚Ä¶but I guess it‚Äôs now a mini project I will try to maintain, so other people like myself can just use and that‚Äôs the beauty of open source right?\nTo get some tea simply do asdf plugin add tea github.com/mvaldes14/asdf-tea followed by asdf install tea latest &amp; asdf global tea latest.\nThen you can tea login and go to town.\nHopefully someone out there can benefit from this little project.\nWant to take a peak at the code? Go here"},"projects/babylog-app":{"title":"Baby Tracker Vue App","links":[],"tags":[],"content":"My family recently expanded in numbers so with the arrival of my new baby girl ‚ù§Ô∏è, I had to remember how to wake up every couple hours, change diapers and keep track of her feeds, wet and dirty diapers up until she reaches 3 weeks or so. So when we were discharged from the hospital we were given a book with a log page so that we could track said events, but of course that book became useless when we had to wake up several times throughout the night and split the work between 2 tired and sleepy parents.\nWhich led my wife to use her cell phone to start writting down the events and that would solve the problem right?. Well what if I was the one who woke up, noticed the diaper and even fed her a bottle?‚Ä¶ I would try to remember the type of event and time so she could write it down when she was awake but that model failed in less than 12 hours as I would start to mix hours and events.\nSo I guess it was time to find an app to use and give away my data to a 3rd party right?!?!. Hell no, as a software engineer I could create something quick and simple that would not require me to hand over any data, personal info or anything at all, which also gives me the freedom to create something that just contains the features me and the wife need.\nOn a fresh morning while drinking coffee the Javascript game was on, and I started coding away.\nThe Stack\nSince we needed something mobile friendly and my Flutter knowledge is very limited right now, I settled with spinning up a web app, make it a PWA and boom!. Native like experience.\nFor the stack I ended up picking these:\n\nVueJS cause it rocks, version 2 since it has better support for 3rd party libraries\nBuefy because I‚Äôm bad at CSS\nFirebase Firestore to keep the data synced between our phones\n\nThe application\nThe UI is pretty simple, a single page application with vue-router rendering whatever the current route is defined as, overall 3 simple pages.\n\nAdd events and show the latest X number of events\nThe entire Log saved in the firestore db\nSome overall stats.\n\nSince this was going to be used in a mobile environment I‚Äôve tried to replicate what a bottom bar with big buttons would do to switch screens but of course since it‚Äôs not native and I‚Äôm pretty bad at CSS it worked, but I guess something better could be done with more time, knowledge and effort.\nThis application had a lifecycle of 3-4 weeks so I really didnt‚Äô want to burn my energy and time, specially since I‚Äôm already behind on sleep and feel pretty tired while my body gets used to this new workflow‚Ä¶so once the kiddo turns 4 weeks the app will most likely die.\nMain Page\nA Simple Navigation bar with 3 buttons allowing you to add the events based on their type (feed/solid/liquid) with emojis because it makes it look good and modern am i right?!. Each click pushes an event and saves it on firestore with the current time.\nContains a small table component that only shows the latest X number of events, adjustable via attributes per component.\n\nIt also makes use of an optional button that displays an extra component that gives you the option to add a past event. Useful if you missed couple events.\n\nLog Page\nA simple sortable table that shows you every single event inside the database. With an optional button that deletes the current event you click on, this one is fairly simple.\nOne takeaway in here would be to maybe paginate the table so that the bottom navigation bar does not get lost in the scrollbar if the table gets too big.\n\nStats Page\nAs mentioned at the very begining of the post, we needed to know how many wet/dirty and feeds the baby had and since counting those events in the main table was a pain, this final stats page basically served as an aggregation showing you totals for the current day, yesterday and everything else in the database.\n\nConclusion\nOverall, the application did what we needed it to do. Got us through the first few weeks and actually helped us find out that our baby girl was not getting enough liquids, which turned into a doctor visit. So yay technology!.\nFrom a tech side, I have never worked with Firebase or similar products before which are pretty cool and easy to use, I know there are OSS alternatives to Firebase like Supabase or Appwrite so I‚Äôm pretty interested in trying those out, specially since you can selfhost them!!.\nThe entire code for the application can be found in my git repo, feel free to browse it and reuse it.\nI know that the firebase configuration should be kept as a secret but by the time you read this, the application will no longer have the backend enabled so nothing harmful is exposed üòó\nYou can visit the finished project here\nAdios üëã"},"projects/meal-notifier":{"title":"Meal Notifier with Go","links":[],"tags":[],"content":"Great teachers on the internet preach that in order to learn a new language effectively, you have to build something with it. Also, those great teachers are pretty much tech Twitter, which can be toxic and polarizing. So take their lessons with a grain of salt.\nAnyway, I‚Äôve been trying to learn Golang for a while, done some courses, taken notes on it, and even got stuck in tutorial hell for some time until the problem to practice the language presented itself.\nWhat is my kid going to have for lunch/breakfast today?\nAs a software engineer, I started the journey to see how to even answer that question, which led me to finding the school site that basically has a calendar with all the options available for kids to pick from and for parents to be aware of their kids‚Äô diet‚Ä¶ that could‚Äôve been it‚Ä¶ no need to do anything else and move on with my life, but there‚Äôs no automation or a way for me to get that information delivered every day at specific times. I‚Äôm not a fan of logging into something just to get a glimpse of some data.\nI almost gave up, but of course, knowing how the web works, I figured, what is feeding this page? Is there some sort of API underneath? If not, maybe I can do some good ol‚Äô scraping?\nAn API was hiding in there after all; I found it by checking the Network tab while loading the page. I found an endpoint that basically feeds the entire site, and you can filter down to specific days or school districts. The endpoint looks something like this\napi.mealviewer.com/api/v4/\nYou can then pass specific parameters to filter your specific school from the district and couple calls later you end up with a response that contains a lot of data, things like calories and nutritional information which is pretty nice, but what I‚Äôm after is the block that contains what is actually for both: Breakfast and Lunch.\n  &quot;id&quot;: 620564,\n  &quot;calculatedPortionSize&quot;: 1.0,\n  &quot;object&quot;: &quot;foodItem&quot;,\n  &quot;menu_Name&quot;: &quot;Elementary Breakfast&quot;,\n  &quot;item_Order_Id&quot;: 1,\n  &quot;block_Name&quot;: &quot;Breakfast&quot;, // Type of meal\n  &quot;block_Id&quot;: 1998,\n  &quot;block_Type&quot;: &quot;menu&quot;,\n  &quot;menu_Block_Date&quot;: &quot;2024-04-29T00:00:00&quot;, // When is this meal served\n  &quot;location_Id&quot;: 13595,\n  &quot;imageFileName&quot;: &quot;633/1AKM2B0O7ht.png&quot;,\n  &quot;item_Id&quot;: 620564,\n  &quot;item_Name&quot;: &quot;French Toast Bites&quot;, // Informaiton we need\n  &quot;nutritionals&quot;: [] // could be used if your kid has specific allergies\n \nKnowing how the data looks like and what we need from it, we can then start doing some Go!\nThe design for this mini project is quite easy:\n\nQuery the API filtering down the school and date.\nParse the response from the API\nForm a new structure on how I want the data to look like\nPrint out the end structure\n\nThat‚Äôs pretty much it, yes?. A simple Golang script that does an HTTP request and prints out something‚Ä¶which is a good starting point but of course that doesn‚Äôt solve the automation and notification portion and since I like to overengineer my life we will also turn this into a Kubernetes cronjob that posts a message to my telegram channel. So we will also need the following:\n\nKubernetes Namespace and Cronjob to launch it on a schedule\nDockerfile to build the image for the script/app\nCICD to upload the image to an image registry (DockerHub)\nCode to include sending data to Telegram\nSecrets to obscure details like my kids school name and the tokens for telegram (Doppler)\n\nBuild stuff time\n\nTLDR: If you just want to see the finished thing check out the repos meal-notifier and k8s-manifest\n\nSo we start by simply building the URL with the parameters needed.\nNote: on the API you need to pass the date range, so it knows when to pull from and since I only need today‚Äôs meal information we provide it twice.\ntoday := time.Now().Format(&quot;1-2-2006&quot;)\nbaseURL := os.Getenv(&quot;BASE_URL&quot;)\nif !strings.HasPrefix(baseURL, &quot;http&quot;) {\n\treturn &quot;URL Not provided&quot;\n}\n \nvar url = fmt.Sprintf(&quot;%s/%s/%s/0&quot;, baseURL, today, today)\n \nSend the request and parse the response, so it matches a struct. In this case I used online tools that generate a struct out of a JSON. The actual response was huge, so I removed a lot of things that were not needed, so we end up with a pretty slim object/type.\ntype response struct { // Trimmed down version of the response with just needed fields\n    MenuSchedules []struct {\n        MenuBlocks []struct {\n            BlockName         string `json:&quot;blockName&quot;`\n            ScheduledDate     string `json:&quot;scheduledDate&quot;`\n            CafeteriaLineList struct {\n                Data []struct {\n                    Name         string `json:&quot;name&quot;`\n                    FoodItemList struct {\n                        Data []struct {\n                            LocationName string `json:&quot;location_name&quot;`\n                            ItemName     string `json:&quot;item_Name&quot;`\n                            Description  string `json:&quot;description&quot;`\n                        }\n                    } \n                }\n            } \n        } \n    } \n}\n...\n \nreq, err := http.Get(url) // Send request\n\tif err != nil {\n\t\tfmt.Println(err)\n\t\treturn &quot;&quot;\n\t}\n \n\tif req.StatusCode != 200 {\n\t\tfmt.Println(&quot;Error: &quot;, req.StatusCode)\n\t\treturn &quot;&quot;\n\t}\n \n\tvar response response\n\tdata, err := io.ReadAll(req.Body)\n \n\tjson.Unmarshal(data, &amp;response) // Parse response using struct above\n \n\tdefer req.Body.Close()\nOnce we have our response in a Golang struct we can then start the madness and iterate over it, since the API returns a pretty extensive object with a lot of nested lists, several loops were required to get to the information we are after.\nI‚Äôm sure there are more efficient and probably better ways to achieve this, but my primitive brain just went with something simple.\nfor _, menu := range response.MenuSchedules {\n\t\tfor _, block := range menu.MenuBlocks {\n\t\t\tfor _, line := range block.CafeteriaLineList.Data {\n\t\t\t\tfor _, item := range line.FoodItemList.Data {\n\t\t\t\t\tif item.LocationName == &quot;CRES- Alternate&quot; {\n\t\t\t\t\t\tcontinue\n\t\t\t\t\t}\n\t\t\t\t\tswitch block.BlockName {\n\t\t\t\t\tcase &quot;Breakfast&quot;:\n\t\t\t\t\t\tbreakfast := meal{\n\t\t\t\t\t\t\tType: &quot;Breakfast&quot;,\n\t\t\t\t\t\t\tItem: item.ItemName,\n\t\t\t\t\t\t}\n\t\t\t\t\t\tmessage.Meals = append(message.Meals, breakfast)\n\t\t\t\t\tcase &quot;Lunch&quot;:\n\t\t\t\t\t\tlunch := meal{\n\t\t\t\t\t\t\tType: &quot;Lunch&quot;,\n\t\t\t\t\t\t\tItem: item.ItemName,\n\t\t\t\t\t\t}\n\t\t\t\t\t\tmessage.Meals = append(message.Meals, lunch)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n \n\tvar payload string\n\tpayload += fmt.Sprintf(&quot;Today is: %s\\n&quot;, time.Now().Format(&quot;2006-01-02&quot;))\n\tfor _, meal := range message.Meals {\n\t\tpayload += fmt.Sprintf(&quot;For %s: %s\\n&quot;, meal.Type, meal.Item)\n\t}\nWith the payload built we can simply then just pass this along to another function that sends the message to Telegram.\ntoken := os.Getenv(&quot;TELEGRAM_HOMELAB_TOKEN&quot;)\nchatID := os.Getenv(&quot;TELEGRAM_CHAT_ID&quot;)\n \nif token == &quot;&quot; || chatID == &quot;&quot; {\n\tfmt.Println(&quot;Missing token or chat id&quot;)\n\treturn\n}\n \nvar url = fmt.Sprintf(&quot;api.telegram.org/bot%s/sendMessage&quot;, token)\nbody, _ := json.Marshal(map[string]string{\n\t&quot;chat_id&quot;: chatID,\n\t&quot;text&quot;:    msg,\n})\nreq, err := http.Post(url, &quot;application/json&quot;, bytes.NewBuffer(body))\nif err != nil {\n\treturn\n}\ndefer req.Body.Close()\nThat pretty much covers the application.\nLike I mentioned, it‚Äôs a pretty simple get/post request script. This could‚Äôve been done in bash am I right!?!?!.\nSome Devops‚Äôing?\nWith the product built we then proceed to make sure this runs on a schedule and that it can pull secrets from my preferred provider Doppler.\nFirst thing we need is to build the container image, which is pretty easy, but we are adding the Doppler CLI on it so when the container boots it can connect to Doppler using a service account token and inject those secrets at.\nFROM golang:1.21.5-alpine\n \nRUN wget -q -t3 &#039;packages.doppler.com/public/cli/rsa.8004D9FF50437357.key&#039; -O /etc/apk/keys/cli@doppler-8004D9FF50437357.rsa.pub &amp;&amp; \\\n    echo &#039;packages.doppler.com/public/cli/alpine/any-version/main&#039; | tee -a /etc/apk/repositories &amp;&amp; \\\n    apk add doppler\n \nWORKDIR /app\n \nCOPY . /app\n \nRUN go build -o meal-notifier .\n \nENTRYPOINT [&quot;doppler&quot;,&quot;run&quot;,&quot;--&quot;,&quot;./meal-notifier&quot;]\nNow that the image can be built, we do some CI/CD to make sure this gets built and published to DockerHub. The action will be pretty simple as it will just install docker, build the image and push it out. Do note your repo will need to have the secrets, so it knows how to connect to the hub.\n steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n      - name: Set up QEMU\n        uses: docker/setup-qemu-action@v3\n      - name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v3\n      - name: Login to Docker Hub\n        uses: docker/login-action@v3\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n      - name: Docker Build and Push\n        uses: docker/build-push-action@v5\n        with:\n          push: true\n          tags: rorix/meal-notifier:latest\nThe final step is to make this run in Kubernetes, because we like to complicate our life right‚Ä¶ this could‚Äôve been in a cronjob on one of my Homelab machines but NO we like the Kubes!. The manifest will simply generate the namespace and define the cronjob to run and when to do it.\nKey thing is that the container needs to know the Doppler service account token, so it can retrieve those secrets, so that token resides within the cluster and gets pulled down by the cronjob.\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: cronjobs\n---\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: meal-notifier \n  namespace: cronjobs \nspec:\n  schedule: &quot;0 7 * * 1-5&quot;\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: meal-notifier\n            image: rorix/meal-notifier:latest\n            env:\n            - name: DOPPLER_TOKEN\n              valueFrom:\n                secretKeyRef:\n                  name: cronjob-secrets\n                  key: DOPPLER_TOKEN\n          restartPolicy: OnFailure\nWith everything said and done, the cronjob will execute and now every Monday through Friday I will have a nice message on Telegram that looks like this:\nNotification from Telegram\n\nCronbjob Execution.\n\nConclusion\nLearn by doing is the way to go!. And this was my first ‚Äúreal‚Äù project using Go so building something so simple took me couple hours just to understand how Go manages things and what methods were the ones I needed to use.\nI‚Äôm quite happy with how it turned out, and it gave me the fuel needed to trying more things with the language.\nSo now I‚Äôm waiting for the next ‚Äúproblem‚Äù to present itself, so I can smack it with some Go!.\nHope you liked it.\nSee ya on the next one.\nAdios üëã"},"projects/terraform-nvim":{"title":"Navigating Terraform manifests in Neovim","links":[],"tags":[],"content":"Part of my work requires me to work with terraform‚Ä¶a lot of terraform since everything has to be done via IaC, or it simply doesn‚Äôt go through AWS, so since my editor of choice is Neovim and anyone can basically create any extension to do whatever‚Ä¶I figured why not have a dedicated terraform plugin to do some planing and exploring?!.\nSo with that mindset terraform.nvim was born.\nIt all started with the idea of‚Ä¶\nWhat if I could preview my plans within the editor like in a floating window?, something like that is pretty easy to do in native Neovim with just opening a new terminal buffer on a side and literally running whatever you wanted.\nBut that isn‚Äôt fancy, and you got to switch between those buffers to go back and edit code, then switch again and re-run your plan. You can see how this can get annoying really fast.\nSo what if a new floating term would open up and run that plan for you? And that all could be invoked in a key bind? - :TerraformPlan became a thing you could do.\nOnce that was working, the next step was finding a way to navigate an existing code base that contained hundreds of resources confined within the working directory‚Ä¶one idea was to just ripgrep and find things, but there‚Äôs also a plugin called Telescope, and it‚Äôs kind of my favorite thing about Neovim and you can make ‚Äúextensions‚Äù for it. So you see where this is going.\nWhat if you could navigate your entire codebase grabbing it directly from the terraform state and present it in a little window that could let you fuzzy find things?; Also what if that window could show you how that current resource in your cursor looked like in the actual provider?. And same as before you could basically bind that to a key?.\nThat‚Äôs how :TerraformExplore was born. And it‚Äôs probably the thing I like the most about the plugin, having the ability to quickly find and edit resources is just amazing. Specially if your manifests contain couple resources per file and your current working directory has more than 30 files‚Ä¶ It becomes a nightmare to fuzzy find and locate things effectively.\nSome screenshots on how it all works\nTerraform Plan telling me what will change.\n\nTerraform Explore without a search parameter, notice how the state shows up in the previewer\n\nOnce you find the resource you can simply ‚ÄúEnter‚Äù on it and the plugin will take you directly to the line that contains that resource, so you can edit it.\n\nThere are still couple things I would like to add to the plugin like:\n\nStarting your terraform project if not initialized\nTerraform Apply with a confirmation pop up window\nTune up the performance as terraform plan can take some time depending on the number of resources in state, maybe cache it somewhere?\nTerraform Module support, picking a resource that is a module is tricky\n\nAs with any other open source project and specially Neovim plugins, the community feedback is always welcome and someone else might find more use cases for something like this.\nIf not I know for sure it will help me at work.\nNot to mention that this is my very first plugin, and it was a blast to make, learned a lot on how Neovim internal kind of work and how to leverage buffers and other plugins to build upon.\n10/10 would recommend anyone using Neovim to build a plugin just to learn.\nFun fact, doing this plugin made me feel comfortable to tackle a linter for nvim-lint as a replacement for null-ls and got my PR approved and merged, quite happy about it.\nIf you find this useful don‚Äôt forget to give it a star over at GitHub"},"projects/vue-composer":{"title":"Docker-compose with vue","links":["[https:/github.com/mvaldes14/vue-composer](https:/github.com/mvaldes14/vue-composer)","[https:/github.com/mvaldes14/vue-composer/packages/246204](https:/github.com/mvaldes14/vue-composer/packages/246204)"],"tags":[],"content":"Due to COVID19 I finally had time to finish some of my outstanding courses, one of those was VueJs so figured i would apply it to something useful instead of doing the regular to-do apps. Which have nothing bad but they are repetitive and well, not fun to be honest.\nSo that being said, had a fun idea of what if I could assemble my basic docker-compose files using a web UI. I know there‚Äôs plugins and things that help you template out the file but I wanted something that you could stack upon and copy to deploy to your docker swarm cluster, so vue composer was born.\nThe Setup\nDISCLAIMER: I‚Äôm Terrible at design or UX for that matter and also a bit lazy so when I saw how vuetify styles the components look super nice and how easy they were to manipulate, figured why not?\n\nInstall vue-cli\nInstall Vuetify\nSpin up a project and lets get started.\n\nThe components\nThe application basically uses 6 components.\n\nThe Home component, that just holds everything below.\nThe Navbar, self explanatory\nThe Footer, same deal\nThe Main Card component that holds 2 sub-components which are the smaller cards\nThe form component that collects the data from the user\nThe display component that shows what the user decided to save\n\nThe first 3 are sort of plain and boring so i‚Äôll skip them over, if you are curious you can checkout the code in the repo Github\nMain.vue\nComponent to simple display a 2 column pane that shows the form for users to enter their data in and the results once they click on ‚ÄúAdd‚Äù. The HTML portion is making use of Vuetify components and the main logic just acts as a middleman to pass data between components.\nDo want to point out that by the time i created this project I was unaware of things like the Vue Data bus or Vuex so i didn‚Äôt make use of them.\nHere we simply register the components and create this middleman object along with a function to populate it so it can be passed.\n&lt;script&gt;\nimport Service from &quot;./Service&quot;;\nimport Compose from &quot;./Compose&quot;;\nexport default {\nname: &quot;Services&quot;,\ncomponents: {\n    Service,\n    Compose,\n},\ndata: function() {\n    return {\n    newService: {},\n    };\n},\nmethods: {\n    addService: function(data) {\n    this.newService = Object.assign({}, data);\n    },\n},\n};\n&lt;/script&gt;\nService.vue\nThis component captures the user input, it leverages Vuetify to render nicely and we simply tie in the input boxes to objects using vue-directives.\nIn here we initially declare the number of services, dependencies, ports, etc. As well as create the main object that will be passed to the parent Main component. Also adding some methods to push and clear the data so the user could add new service objects.\n&lt;script&gt;\nexport default {\nname: &quot;Service&quot;,\ndata: function() {\n    return {\n    numberOfDependencies: 1,\n    numberOfPorts: 1,\n    numberOfVolumes: 1,\n    numberOfEnvironment: 1,\n    serviceObject: {\n        serviceName: &quot;&quot;,\n        imageName: &quot;&quot;,\n        containerName: &quot;&quot;,\n        dependsOn: [],\n        environment: [],\n        ports: [],\n        volumes: [],\n    },\n    };\n},\nmethods: {\n    add: function() {\n    this.$emit(&quot;service-added&quot;, this.serviceObject);\n    },\n    clear: function() {\n    var obj = this.serviceObject;\n    for (const prop of Object.getOwnPropertyNames(obj)) {\n        obj[prop] = &quot;&quot;;\n    }\n    obj[&quot;dependsOn&quot;] = [];\n    obj[&quot;environment&quot;] = [];\n    obj[&quot;ports&quot;] = [];\n    obj[&quot;volumes&quot;] = [];\n    },\n},\n};\nThe tricky part here for me as a novice was on how to make the value of the property object actually clear on its own so when a user pushes the button to clear the data it would do that without removing the actual object. This was something I had to learn the hard way as the data was simply being passed by reference, meaning the value in memory/cache or whatever JavaScript uses and a new object was not being instantiated again so yeah‚Ä¶ next time make sure that you make a copy of the object instead of passing it around like i did.\nCompose.vue\nThe last component simply render the object that is passed to it as the user formed it. This component is mostly a bunch of v-for iterating over the number of arguments in the object as the user defined them. Example of how the dependencies are rendered.\n&lt;div v-if=&quot;serv.dependsOn.length &gt; 0&quot;&gt;\n  &lt;strong&gt; depends_on: &lt;/strong&gt;\n  &lt;div\n    class=&quot;sublist&quot;\n    :key=&quot;dep&quot;\n    v-for=&quot;dep in serv.dependsOn&quot;\n    style=&quot;color:white&quot;\n  &gt;\n    - {{ dep }}\n  &lt;/div&gt;\n&lt;/div&gt;\nThe logic for this one is quite simple, it just pushes the object it receives from the parent Main.vue component and pushes it to the list.\n&lt;script&gt;\n    export default {\n    name: &quot;Compose&quot;,\n    props: {\n        newService: Object,\n    },\n    data: function() {\n        return {\n        serviceList: [],\n        };\n    },\n    watch: {\n        newService() {\n        this.serviceList.push(this.newService);\n        },\n    },\n    };\n&lt;/script&gt;\nIn Conclusion\nThis was a fun little project that helped me realize that I‚Äôm not that lost in terms of JavaScript, I‚Äôm no expert of course but i believe I‚Äôm slowly getting decent at it. Also VueJs has been on my radar for quite a while so this was a great opportunity for me to try it, now i need to wrap up the course, starting playing with Vuex and Vue router‚Ä¶ and maybe, just maybe even try some ReactJs i know that most people love it but i guess at the time i was reading documentation my overall JavaScript knowledge was so poor and JSX looked so complicated it honestly freaked me out a bit.\nAnother cool thing I had the chance to play with was deploying the application to a free service like Netlify as well as using the Github Registry to push the actual image so it can be pulled and used as a container in Docker or K8s.\nVue-composer Docker Image\nGithub Repo for this post\nFinally, down the road i would like to improve on the little tool. For instance i would like to do validation so when a user enters data the tools makes sure it is in the proper format for things like ports and volumes. If Dockerhub offers an API maybe even check if the image exists, etc. Lots of things that could be done but for a novice like myself this was a good start.\nYou can view the finished project here\nHope you liked it, see ya on the next one."},"test":{"title":"This is a test","links":[],"tags":[],"content":""}}